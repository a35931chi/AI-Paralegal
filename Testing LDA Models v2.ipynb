{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#visualization libraries\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = 'C:/Program Files/Java/jdk-10.0.1/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import itertools as it\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import corpus/docket texts from html to pandas DataFrame\n",
    "def grab_docket_test():\n",
    "    files = []\n",
    "    #get all .html files in the folder (all docket files are in .html)\n",
    "    for file in os.listdir('docket_texts/test/'):\n",
    "        if file.endswith('.html'):\n",
    "            files.append(os.path.join('docket_texts/test/', file))\n",
    "\n",
    "    df_docket_texts = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(files)): #gather all docket texts\n",
    "    #for i in [0, 1]: #for testing purposes\n",
    "        \n",
    "        content = codecs.open(files[i], 'r', 'utf-8').read()\n",
    "        #use beautiful soup to get the case ID\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        case_id = str(soup.find_all('h3'))    \n",
    "        bookmark1 = case_id.find('CASE #:') + len('CASE #:')\n",
    "        bookmark2 = case_id.find('</h3>')\n",
    "        case_id = case_id[bookmark1:bookmark2]\n",
    "\n",
    "        #use pandas to grab tables in the html files\n",
    "        docket_tables = pd.read_html(content)\n",
    "\n",
    "        #error checking: gotta do this because there's different length of docket_list/\n",
    "        #usually docket texts are in docket_list[3], but not always\n",
    "        n = 0\n",
    "        while docket_tables[n].isin(['Docket Text']).sum().sum() == 0:\n",
    "            #print(n, docket_tables[n].isin(['Docket Text']).sum().sum())\n",
    "            n += 1\n",
    "                        \n",
    "        #print(i, files[i])\n",
    "        #print(docket_tables[n].head())\n",
    "\n",
    "        #docket_tables[n] is the docket text table\n",
    "        new_header = docket_tables[n].iloc[0]\n",
    "        docket_tables[n] = docket_tables[n][1:]\n",
    "        docket_tables[n].columns = new_header\n",
    "        \n",
    "        docket_tables[n]['#'] = pd.to_numeric(docket_tables[n]['#'],\n",
    "                                              downcast = 'signed', errors = 'coerce')\n",
    "        docket_tables[n]['Date Filed'] = pd.to_datetime(docket_tables[n]['Date Filed'])\n",
    "        docket_tables[n]['Case ID'] = case_id\n",
    "\n",
    "        df_docket_texts = pd.concat([df_docket_texts, docket_tables[n]])\n",
    "    #reorder a column\n",
    "    cols = list(df_docket_texts.columns)\n",
    "    df_docket_texts = df_docket_texts[[cols[-1]] + cols[:-1]]\n",
    "    \n",
    "    print('current docket text table size/shape: {}'.format(df_docket_texts.shape))\n",
    "    return df_docket_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current docket text table size/shape: (1804, 4)\n",
      "docket text 0\n",
      "FILING ERROR - DEFICIENT PLEADING - FILED AGAINST PARTY ERROR COMPLAINT against All Defendants. (Filing Fee $ 400.00, Receipt Number 0208-11793625)Document filed by Majid Soueidan.(Rosen, Samuel) Modified on 1/5/2016 (pc). (Entered: 01/04/2016) \n",
      "\n",
      "docket text 1\n",
      "FILING ERROR PDF ERROR CIVIL COVER SHEET filed. (Rosen, Samuel) Modified on 1/5/2016 (pc). (Entered: 01/04/2016) \n",
      "\n",
      "docket text 2\n",
      "REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-EASTERN CORPORATION, BRAD PEDERSEN, ROBERT J. KELLY, NELSON OBUS, WILLIAM M. SHOCKLEY, and SERGE DUPUIS, re: 1 Complaint. Document filed by Majid Soueidan. (Rosen, Samuel) (Entered: 01/04/2016) \n",
      "\n",
      "docket text 3\n",
      "RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Corporate Parent. Document filed by Majid Soueidan.(Rosen, Samuel) (Entered: 01/04/2016) \n",
      "\n",
      "docket text 4\n",
      "***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEADING. Notice to Attorney Samuel Kenneth Rosen to RE-FILE Document No. 1 Complaint,. The filing is deficient for the following reason(s): the All Defendant radio button was selected. Re-file the pleading using the event type Complaint found under the event list Complaints and Other Initiating Documents - attach the correct signed PDF - select the individually named filer/filers - select the individually named party/parties the pleading is against. (pc) (Entered: 01/05/2016) \n",
      "\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = grab_docket_test()\n",
    "docket_original = list(df['Docket Text'])\n",
    "for i in range(5):\n",
    "    print('docket text {}'.format(i))\n",
    "    print(docket_original[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_to_model = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\classifiers\\english.all.3class.distsim.crf.ser.gz'\n",
    "path_to_jar = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\stanford-ner.jar'\n",
    "tagger = StanfordNERTagger(path_to_model, path_to_jar = path_to_jar)\n",
    "\n",
    "output = []\n",
    "#length = 100 \n",
    "length = len(docket_original)\n",
    "for i in range(length):\n",
    "    org_str = []\n",
    "    name_str = []\n",
    "    stripped_str1 = []\n",
    "    stripped_str2 = []\n",
    "    tokens = nltk.tokenize.word_tokenize(docket_original[i])\n",
    "    for label in tagger.tag(tokens):\n",
    "        #print(label)\n",
    "        if label[1] == 'ORGANIZATION':\n",
    "            org_str.append(label[0])\n",
    "            stripped_str1.append('-ORG-')\n",
    "        elif label[1] == 'PERSON':\n",
    "            name_str.append(label[0])\n",
    "            stripped_str1.append('-NAME-')\n",
    "        else:\n",
    "            stripped_str1.append(label[0])\n",
    "            stripped_str2.append(label[0])\n",
    "    \n",
    "    output.append([docket_original[i],\n",
    "                   ' '.join(org_str),\n",
    "                   ' '.join(name_str),\n",
    "                   ' '.join(stripped_str1),\n",
    "                   ' '.join(stripped_str2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df = pd.DataFrame(output, columns = ['Original Docket Text', 'Organization Portion', 'Name Portion', \n",
    "                                         'Identifying Org and Name', 'Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = NER_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Original Docket Text Organization Portion  \\\n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...                        \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...                        \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...                        \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...                        \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...                        \n",
      "\n",
      "                                        Name Portion  \\\n",
      "0                        Majid Soueidan Rosen Samuel   \n",
      "1                                       Rosen Samuel   \n",
      "2  BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...   \n",
      "3                        Majid Soueidan Rosen Samuel   \n",
      "4                               Samuel Kenneth Rosen   \n",
      "\n",
      "                            Identifying Org and Name  \\\n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
      "\n",
      "                               Stripped Org and Name  \n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...  \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...  \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...  \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...  \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...  \n"
     ]
    }
   ],
   "source": [
    "print(new_df.head())\n",
    "docket_text_list = list(new_df['Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess1(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('(s)', 's')\n",
    "    text = text.replace(\"'s\", 's')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.replace('&', ' ')\n",
    "    return text\n",
    "\n",
    "def text_preprocess2(text):\n",
    "    text = text.replace('.', '')\n",
    "    return text\n",
    "\n",
    "def remove_stop(sentence):\n",
    "    output = []\n",
    "    for word in sentence.split():\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            output.append(word)\n",
    "    return ' '.join(output)\n",
    "\n",
    "keywords = pd.read_csv('docket_texts/keywords.csv', header = None)\n",
    "keywords.columns = ['keywords']\n",
    "keyword_list = list(keywords['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILING ERROR PDF ERROR CIVIL COVER SHEET filed . ( , ) Modified on 152016 ( pc ) . ( Entered : 01042016 )\n",
      "filing error pdf error civil cover sheet filed   modified on 152016   \n",
      "1804\n"
     ]
    }
   ],
   "source": [
    "print(docket_text_list[1])\n",
    "docket_text_list = [text_preprocess1(sentence).lower() for sentence in docket_text_list]\n",
    "docket_text_list = [text_preprocess2(sentence) for sentence in docket_text_list]\n",
    "print(docket_text_list[1])\n",
    "print(len(docket_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(remove_stop(sent)) for sent in sentences]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class LemmatizationWithPOSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_wordnet_pos(self,treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def pos_tag(self,tokens):\n",
    "        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "\n",
    "        # lemmatization using pos tagg   \n",
    "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
    "        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
    "        return pos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = Splitter()\n",
    "lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n",
    "\n",
    "lemma_docket_text_list = []\n",
    "for docket_text in docket_text_list:\n",
    "    #step 1 split document into sentence followed by tokenization\n",
    "    tokens = splitter.split(docket_text)\n",
    "\n",
    "    #step 2 lemmatization using pos tagger \n",
    "    lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "    lemma_docket_text_list.append(lemma_pos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804\n",
      "1\n",
      "13\n",
      "('filing', 'file', ['VBG'])\n",
      "filing\n"
     ]
    }
   ],
   "source": [
    "print(len(lemma_docket_text_list)) #docket text document level\n",
    "print(len(lemma_docket_text_list[0])) #docket text sentence level\n",
    "print(len(lemma_docket_text_list[0][0])) #docket text word level\n",
    "print(lemma_docket_text_list[0][0][0]) #docket text token level\n",
    "print(lemma_docket_text_list[0][0][0][0]) #docket text tuple level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do a collection of what we have\n",
    "collection = {}\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    for sentence in lemma_pos_token:\n",
    "        for token in sentence:\n",
    "            #print(token[2][0])\n",
    "            if token[2][0] not in list(collection.keys()):\n",
    "                collection[token[2][0]] = []\n",
    "                collection[token[2][0]].append(token[1])\n",
    "            else:\n",
    "                if token[1] not in collection[token[2][0]]:\n",
    "                    collection[token[2][0]].append(token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(dict([ (k, pd.Series(v)) for k, v in collection.items()])).to_csv('NLP_pos.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['file error deficient plead file error complaint document file'], ['file error pdf error civil cover sheet file'], ['request issuance summons breezeeastern corporation complaint document file'], ['rule corporate disclosure corporate parent document file'], ['notice deficient pleading notice refile document complaint file deficient reason radio button select refile plead use event type complaint find event list complaint initiate document correct pdf select individually name filerfilers select individually name partyparties plead'], ['notice modification partyparties partyparties reasonreasons name contain typographical error text omit'], ['notice deficient civil cover sheet notice refile document civil cover sheet filing deficient reason citizenship principal fill despite federal question jurisdiction'], ['electronic summons issue'], ['notice open statistical error correction notice open statistical erroneously selectedentered fee status code due correction entry fee status code pd'], ['open initial assignment notice aboveentitled assign please download assign locate http nysduscourtsgovjudgesdistrict responsible courtesy individual practice please download rule instruction locate http nysduscourtsgovecf_filingphp']]\n",
      "Wall time: 83.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "remove_pos = [\"``\", \"NNPS\", \"NNP\", \"CD\", '#', '$', \"''\", \",\", \"0\", \":\"]\n",
    "remove_word = [\"'s\", \"judge\", \"party\", \"defendant\", \"ex\", \"plantiff\", \"shall\", \"date\", \"b\", \"exhibit\", \"pennsylvania\", \"sign_judge\", \n",
    "               \"Inc..\", \"inc..\", \"llc\", \"'\", \"[_]\", \"action\", \"clerk\", \"july\", \"kw\", \"regard\", \"sac\", \"attachment\", \"c.d\", \"cal\", \"case\", \"cd\", \"l.p.\", \n",
    "               \"claim\", \"copy\", \"court\", \"direct\", \"form\", \"hereby\", \"magistrate\", \"p.c\", \"pl\", \"plaintiff\", \"regard\", \"sign\", \"time\", \"mr.\", \n",
    "               \"docket\", \"follow\", \"set\", \"matter\" \"agreement\" \"proceeding\", \"cotton\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \n",
    "               \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "               \"agreement\", \"v.\", \"place_vault\", \"modify\", \"fund\", \"associated\", \"provide\", \"material\", \"amount\", \"accordingly\", \"additional\", \n",
    "               \"second\", \"esq\", \"transmission\", \"g.c.\", \"seal\", \"review\", \"honor\", \"submit\", \"counsel\", \"witness\", \"civ\", \"first\", \"ltd..\", \"enter\", \n",
    "               \"stay\", \"forth\", \"matter\", \"whether\", \"class\", \"master\", \"information\", \"statement\", \"submission\", \"related\", \"see\", \"make\", \"paper\", \n",
    "               \"brookfield\", \"designate\", \"remain\", \"reportertranscriber\", \"submit\", \"include\", \"mail\", \"fact\", \"refer\", \"take\", \"pursuant\", \"amount\", \n",
    "               \"behalf\", \"I.p..\", \"must\", \"attorney\",\n",
    "               'abovecapitoned', 'attach', 'add', 'concern', 'chamber', 'close', 'district', 'damage', 'later', \n",
    "               'relate', 'return', 'require', 'restriction', 'respect', 'ny', 'seek', 'write', 'expert', 'transcript', \n",
    "               'day', 'h.o', 'damage', 'pre', 'proceeding', 'present', 'page', 'pending', 'p.m.', 'frcp', 'g.c.', 'record', 'r.']\n",
    "\n",
    "    \n",
    "#rebuild corpus\n",
    "docket_texts_output = [] #ultimate output after cleaning\n",
    "\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    docket_text_output = [] \n",
    "    for sentence in lemma_pos_token:\n",
    "        sentence_output = []\n",
    "        for token in sentence:\n",
    "            #print(token[1])\n",
    "            \n",
    "            if token[2][0] not in remove_pos: #if the pos is not in the remove_pos list\n",
    "                if token[1] not in remove_word: #these are the intentionally left out words\n",
    "                    sentence_output.append(token[1]) #append the the sentence\n",
    "        docket_text_output.append(' '.join(sentence_output))\n",
    "    docket_texts_output.append(docket_text_output)\n",
    "print(docket_texts_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>[file error deficient plead file error complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td></td>\n",
       "      <td>Rosen Samuel</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>[file error pdf error civil cover sheet file]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td></td>\n",
       "      <td>BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>[request issuance summons breezeeastern corpor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>[rule corporate disclosure corporate parent do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td></td>\n",
       "      <td>Samuel Kenneth Rosen</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>[notice deficient pleading notice refile docum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text Organization Portion  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...                        \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...                        \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...                        \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...                        \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...                        \n",
       "\n",
       "                                        Name Portion  \\\n",
       "0                        Majid Soueidan Rosen Samuel   \n",
       "1                                       Rosen Samuel   \n",
       "2  BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...   \n",
       "3                        Majid Soueidan Rosen Samuel   \n",
       "4                               Samuel Kenneth Rosen   \n",
       "\n",
       "                            Identifying Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                               Stripped Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \n",
       "0  [file error deficient plead file error complai...  \n",
       "1      [file error pdf error civil cover sheet file]  \n",
       "2  [request issuance summons breezeeastern corpor...  \n",
       "3  [rule corporate disclosure corporate parent do...  \n",
       "4  [notice deficient pleading notice refile docum...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Removed unnecessary POS & vocab'] = pd.Series(docket_texts_output)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_topics_df = pd.read_csv('mannual_topics.csv')\n",
    "manual_topics_df = manual_topics_df.apply(lambda x: x.astype(str).str.lower())\n",
    "manual_topics_dict = manual_topics_df.to_dict('list')\n",
    "for topic in manual_topics_dict.keys():\n",
    "    manual_topics_dict[topic] = [keyword for keyword in manual_topics_dict[topic] if keyword != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mannual_topic_assignment(a_list):\n",
    "    text = ' '.join(a_list).split()\n",
    "    #print(text)\n",
    "    output = []\n",
    "    for topic in manual_topics_dict.keys():\n",
    "        if set(text).intersection(manual_topics_dict[topic]):\n",
    "            output.append(topic)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_texts_output_DT = []\n",
    "topics_DT = []\n",
    "\n",
    "for text in docket_texts_output:\n",
    "    topic = mannual_topic_assignment(text)\n",
    "    if topic != []:\n",
    "        docket_texts_output_DT.append([])\n",
    "        topics_DT.append(topic)\n",
    "    else:\n",
    "        docket_texts_output_DT.append(text)\n",
    "        topics_DT.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Complaints'], [], ['Others', 'Complaints', 'Service of Process'], [], ['Complaints', 'Notices']]\n",
      "[[], ['file error pdf error civil cover sheet file'], [], ['rule corporate disclosure corporate parent document file'], []]\n"
     ]
    }
   ],
   "source": [
    "print(topics_DT[:5])\n",
    "print(docket_texts_output_DT[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Original Docket Text Organization Portion  \\\n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...                        \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...                        \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...                        \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...                        \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...                        \n",
      "\n",
      "                                        Name Portion  \\\n",
      "0                        Majid Soueidan Rosen Samuel   \n",
      "1                                       Rosen Samuel   \n",
      "2  BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...   \n",
      "3                        Majid Soueidan Rosen Samuel   \n",
      "4                               Samuel Kenneth Rosen   \n",
      "\n",
      "                            Identifying Org and Name  \\\n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
      "\n",
      "                               Stripped Org and Name  \\\n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
      "\n",
      "                     Removed unnecessary POS & vocab  \\\n",
      "0  [file error deficient plead file error complai...   \n",
      "1      [file error pdf error civil cover sheet file]   \n",
      "2  [request issuance summons breezeeastern corpor...   \n",
      "3  [rule corporate disclosure corporate parent do...   \n",
      "4  [notice deficient pleading notice refile docum...   \n",
      "\n",
      "                                  DT Topics  \\\n",
      "0                              [Complaints]   \n",
      "1                                        []   \n",
      "2  [Others, Complaints, Service of Process]   \n",
      "3                                        []   \n",
      "4                     [Complaints, Notices]   \n",
      "\n",
      "                  Removed unnecessary POS & vocab DT  \n",
      "0                                                 []  \n",
      "1      [file error pdf error civil cover sheet file]  \n",
      "2                                                 []  \n",
      "3  [rule corporate disclosure corporate parent do...  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "source": [
    "new_df['DT Topics'] = pd.Series(topics_DT)\n",
    "new_df['Removed unnecessary POS & vocab DT'] = pd.Series(docket_texts_output_DT)\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['Complaints']\n",
      "['file error deficient plead file error complaint document file']\n",
      "[]\n",
      "1\n",
      "2\n",
      "['Others', 'Complaints', 'Service of Process']\n",
      "['request issuance summons breezeeastern corporation complaint document file']\n",
      "[]\n",
      "3\n",
      "4\n",
      "['Complaints', 'Notices']\n",
      "['notice deficient pleading notice refile document complaint file deficient reason radio button select refile plead use event type complaint find event list complaint initiate document correct pdf select individually name filerfilers select individually name partyparties plead']\n",
      "[]\n",
      "5\n",
      "['Notices']\n",
      "['notice modification partyparties partyparties reasonreasons name contain typographical error text omit']\n",
      "[]\n",
      "6\n",
      "['Notices']\n",
      "['notice deficient civil cover sheet notice refile document civil cover sheet filing deficient reason citizenship principal fill despite federal question jurisdiction']\n",
      "[]\n",
      "7\n",
      "['Service of Process']\n",
      "['electronic summons issue']\n",
      "[]\n",
      "8\n",
      "['Notices']\n",
      "['notice open statistical error correction notice open statistical erroneously selectedentered fee status code due correction entry fee status code pd']\n",
      "[]\n",
      "9\n",
      "['Notices']\n",
      "['open initial assignment notice aboveentitled assign please download assign locate http nysduscourtsgovjudgesdistrict responsible courtesy individual practice please download rule instruction locate http nysduscourtsgovecf_filingphp']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#print some examples\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    if new_df['DT Topics'].iloc[i] != []:\n",
    "        print(new_df['DT Topics'].iloc[i])\n",
    "        print(new_df['Removed unnecessary POS & vocab'].iloc[i])\n",
    "        print(new_df['Removed unnecessary POS & vocab DT'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_transform(texts):\n",
    "    trigram_output = []\n",
    "\n",
    "    remove_trigram = ['calendar_day', 'court_notice_intend', 'minute_entry_proceeding_hold', 'court_reportertranscriber_abovecaptioned_matter',\n",
    "                      'redaction_calendar_day', 'rule_statement', 'obtain_pacer', 'may_obtain_pacer', 'reportertranscriber_abovecaptioned_matter',\n",
    "                      'redact_transcript_deadline', 'send_chamber', \"official_transcript_notice_give\", \"notice_intent_request\", \"proceed_hold\", \n",
    "                      \"fee_receipt_number\", \"civil_procedure\", \"pursuant_frcp\", \"official_transcript_conference\", \n",
    "                      \"purchase_reportertranscriber_deadline_release\", \"et_al\", \"mail_chamber\", \"transcript_restriction\", \"redaction_transcript\", \n",
    "                      \"transcript_view_public_terminal\", \"transcript_make_remotely\", \"associated_et_al\", \"electronically_available_public_without\", \n",
    "                      \"genesys_id\", \"release_transcript_restriction\", \"adar_bay\", \"redaction_request_due\", \"new_york\", \"official_transcript_conference\", \n",
    "                      \"transcript_make_remotely\", \"transcript_proceeding_conference_hold\", \"redaction_transcript\",\n",
    "                      'affidavit_jr._c.p.a', 'corporate_parent', 'certain_underwriter', 'federal_rule_civil_procedure', 'redaction_request', \n",
    "                      'official_transcript', 'rule_disclosure', 'rule_corporate_disclosure', 'place_vault', 'public_without_redaction_calendar', \n",
    "                      'purchase_deadline_release_transcript', 'transcript_proceeding_hold', 'transcript_remotely_electronically_available']\n",
    "  \n",
    "\n",
    "    for sentence in texts:\n",
    "        unigram_review = []\n",
    "        for word in sentence.split():\n",
    "            unigram_review.append(word)\n",
    "    \n",
    "        #print('Uni: ', unigram_review)\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        #print('Bi: ', bigram_review)\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "        trigram_review = [phrase for phrase in trigram_review if phrase not in remove_trigram]\n",
    "        #print('Tri: ', trigram_review)\n",
    "        trigram_output.append(' '.join(trigram_review))\n",
    "    return trigram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram_model_filepath = 'docket_texts/train/DT/bigram_model_noorgnoname'\n",
    "trigram_model_filepath = 'docket_texts/train/DT/trigram_model_nonamenoorg'\n",
    "\n",
    "bigram_model = Phrases.load(bigram_model_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)\n",
    "\n",
    "new_df['Apply Trigram Phrase Model'] = new_df['Removed unnecessary POS & vocab DT'].apply(trigram_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               []\n",
       "1    [file error pdf error civil_cover_sheet file]\n",
       "2                                               []\n",
       "3                                  [document file]\n",
       "4                                               []\n",
       "Name: Apply Trigram Phrase Model, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Apply Trigram Phrase Model'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write trigram to file, not sure why...\n",
    "trigram_dockets_filepath = 'docket_texts/test/DT/trigram_transformed_dockets_noorgnoname.txt'\n",
    "\n",
    "with codecs.open(trigram_dockets_filepath, 'w', encoding= 'utf_8') as f:\n",
    "    for i in range(len(new_df['Apply Trigram Phrase Model'])):\n",
    "        f.write(' '.join(new_df['Apply Trigram Phrase Model'][i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = 'docket_texts/test/DT/trigram_dict_noorgnoname.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#some dictionary hyperparameters:\n",
    "no_below = 10 #reference is 10\n",
    "no_above = 0.4 #reference is 0.4\n",
    "\n",
    "trigram_reviews = LineSentence(trigram_dockets_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare otrigram_reviewsr too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below = no_below, no_above = no_above) #this step is questionable. May need to change the parameters\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "#trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = 'docket_texts/test/DT/trigram_bow_corpus_noorgnoname.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        #print(review)\n",
    "        #print(trigram_dictionary.doc2bow(review))\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = 'docket_texts/test/DT/trigram_sentences_nonamenoorg.txt'\n",
    "\n",
    "with codecs.open(trigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for trigram_sentence in list(new_df['Apply Trigram Phrase Model']):\n",
    "        #print(' '.join(trigram_sentence))\n",
    "        if \n",
    "        f.write(' '.join(trigram_sentence) + '\\n')\n",
    "        \n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = 'docket_texts/test/DT/trigram_bow_corpus_noorgnoname.mm'\n",
    "trigram_dictionary_filepath = 'docket_texts/test/DT/trigram_dict_noorgnoname.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(263 documents, 25 features, 486 non-zero entries)\n",
      "Wall time: 35.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# generate bag-of-words representations for\n",
    "# all reviews and save them as a matrix\n",
    "MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_sentences_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "print(trigram_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.mmcorpus.MmCorpus at 0x262b6abf188>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_bow_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.52 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#some dictionary hyperparameters:\n",
    "no_below = 10 #reference is 10\n",
    "no_above = 0.4 #reference is 0.4\n",
    "\n",
    "trigram_reviews = LineSentence(trigram_dockets_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare otrigram_reviewsr too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below = no_below, no_above = no_above) #this step is questionable. May need to change the parameters\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying models to docket text test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guessing we are using 4 topics\n",
    "num_topics = 4\n",
    "lda = LdaMulticore.load('docket_texts/train/DT/lda_model_noorgnomodel_' + str(num_topics))\n",
    "train_dict = Dictionary.load('docket_texts/train/DT/trigram_dict_noorgnoname.dict')\n",
    "\n",
    "test_dict = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 minute_entry_hold\n",
      "19 associate\n",
      "6 letter\n",
      "2 affiliate\n",
      "24 receiver\n",
      "16 hold\n",
      "22 bench_trial\n",
      "7 order\n",
      "23 fee\n",
      "5 letter_address\n",
      "0 error\n",
      "15 schedule\n",
      "4 inc\n",
      "10 rule\n",
      "13 motion\n",
      "11 endorsement\n",
      "18 pak6\n",
      "3 create\n",
      "20 motion_limine\n",
      "12 memo_endorsement\n",
      "8 application\n",
      "14 mediation\n",
      "1 deadline\n",
      "21 grant\n",
      "9 discovery\n"
     ]
    }
   ],
   "source": [
    "for new_id, token in test_dict.iteritems():\n",
    "    print(new_id, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_description(docket_text, lda, trigram_dictionary, topic_names, min_topic_freq = 0.05):\n",
    "    '''\n",
    "    accept the processed texts (trigram) of a review and \n",
    "    1) create a bag-of-words representation, \n",
    "    4) create an LDA representation, and\n",
    "    5) print a sorted list of the top topics in the LDA representation\n",
    "    '''\n",
    "    output = []\n",
    "    analyze_this = []\n",
    "    for sentence in docket_text:\n",
    "        analyze_this += sentence.split()\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    review_bow = trigram_dictionary.doc2bow(analyze_this)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    review_lda = lda[review_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    review_lda.sort(key = lambda tup: tup[1], reverse = True)\n",
    "    #print(review_lda)\n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        #print('{:25} {}'.format(topic_names[topic_number], round(freq, 3)))\n",
    "        output.append((topic_names[topic_number], round(freq, 5)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' commented out due to new DT direction\n",
    "# according to Chris' feedback on 2018-5-10\n",
    "topic_names = {0: 'motion to dismiss',\n",
    "               1: 'motion for summary judgment',\n",
    "               2: 'Complaint and motion',\n",
    "               3: 'Amended Complaint and motion'}\n",
    "'''\n",
    "\n",
    "topic_names = {}\n",
    "for i in range(num_topics):\n",
    "    topic_names[i] = 'Topic ' + str(i)\n",
    "\n",
    "topic_summary = []\n",
    "    \n",
    "for docket_text in list(new_df['Apply Trigram Phrase Model']):\n",
    "    #print(docket_text)\n",
    "    if docket_text == []:\n",
    "        topic_summary.append('')\n",
    "    else:\n",
    "        topic_summary.append(lda_description(docket_text, lda, trigram_dictionary, topic_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "      <th>Topic Model Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>[file error deficient plead file error complai...</td>\n",
       "      <td>[Complaints]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td></td>\n",
       "      <td>Rosen Samuel</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>[file error pdf error civil cover sheet file]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[file error pdf error civil cover sheet file]</td>\n",
       "      <td>[file error pdf error civil_cover_sheet file]</td>\n",
       "      <td>[(Topic 2, 0.74803), (Topic 0, 0.08516), (Topi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td></td>\n",
       "      <td>BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>[request issuance summons breezeeastern corpor...</td>\n",
       "      <td>[Others, Complaints, Service of Process]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>[rule corporate disclosure corporate parent do...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[rule corporate disclosure corporate parent do...</td>\n",
       "      <td>[document file]</td>\n",
       "      <td>[(Topic 0, 0.25), (Topic 1, 0.25), (Topic 2, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td></td>\n",
       "      <td>Samuel Kenneth Rosen</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>[notice deficient pleading notice refile docum...</td>\n",
       "      <td>[Complaints, Notices]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text Organization Portion  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...                        \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...                        \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...                        \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...                        \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...                        \n",
       "\n",
       "                                        Name Portion  \\\n",
       "0                        Majid Soueidan Rosen Samuel   \n",
       "1                                       Rosen Samuel   \n",
       "2  BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...   \n",
       "3                        Majid Soueidan Rosen Samuel   \n",
       "4                               Samuel Kenneth Rosen   \n",
       "\n",
       "                            Identifying Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                               Stripped Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \\\n",
       "0  [file error deficient plead file error complai...   \n",
       "1      [file error pdf error civil cover sheet file]   \n",
       "2  [request issuance summons breezeeastern corpor...   \n",
       "3  [rule corporate disclosure corporate parent do...   \n",
       "4  [notice deficient pleading notice refile docum...   \n",
       "\n",
       "                                  DT Topics  \\\n",
       "0                              [Complaints]   \n",
       "1                                        []   \n",
       "2  [Others, Complaints, Service of Process]   \n",
       "3                                        []   \n",
       "4                     [Complaints, Notices]   \n",
       "\n",
       "                  Removed unnecessary POS & vocab DT  \\\n",
       "0                                                 []   \n",
       "1      [file error pdf error civil cover sheet file]   \n",
       "2                                                 []   \n",
       "3  [rule corporate disclosure corporate parent do...   \n",
       "4                                                 []   \n",
       "\n",
       "                      Apply Trigram Phrase Model  \\\n",
       "0                                             []   \n",
       "1  [file error pdf error civil_cover_sheet file]   \n",
       "2                                             []   \n",
       "3                                [document file]   \n",
       "4                                             []   \n",
       "\n",
       "                                  Topic Model Output  \n",
       "0                                                     \n",
       "1  [(Topic 2, 0.74803), (Topic 0, 0.08516), (Topi...  \n",
       "2                                                     \n",
       "3  [(Topic 0, 0.25), (Topic 1, 0.25), (Topic 2, 0...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Topic Model Output'] = topic_summary\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original Docket Text', 'Organization Portion', 'Name Portion',\n",
       "       'Identifying Org and Name', 'Stripped Org and Name',\n",
       "       'Removed unnecessary POS & vocab', 'DT Topics',\n",
       "       'Removed unnecessary POS & vocab DT', 'Apply Trigram Phrase Model',\n",
       "       'Topic Model Output'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[['Original Docket Text', 'Removed unnecessary POS & vocab DT', 'Apply Trigram Phrase Model', \n",
    "        'DT Topics', 'Topic Output']].to_csv('docket_texts/test/DT/verify_this.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
