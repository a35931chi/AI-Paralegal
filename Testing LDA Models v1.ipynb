{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#visualization libraries\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = 'C:/Program Files/Java/jdk-10.0.1/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import itertools as it\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import corpus/docket texts from html to pandas DataFrame\n",
    "def grab_docket_test():\n",
    "    files = []\n",
    "    #get all .html files in the folder (all docket files are in .html)\n",
    "    for file in os.listdir('docket_texts/test/'):\n",
    "        if file.endswith('.html'):\n",
    "            files.append(os.path.join('docket_texts/test/', file))\n",
    "\n",
    "    df_docket_texts = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(files)): #gather all docket texts\n",
    "    #for i in [0, 1]: #for testing purposes\n",
    "        \n",
    "        content = codecs.open(files[i], 'r', 'utf-8').read()\n",
    "        #use beautiful soup to get the case ID\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        case_id = str(soup.find_all('h3'))    \n",
    "        bookmark1 = case_id.find('CASE #:') + len('CASE #:')\n",
    "        bookmark2 = case_id.find('</h3>')\n",
    "        case_id = case_id[bookmark1:bookmark2]\n",
    "\n",
    "        #use pandas to grab tables in the html files\n",
    "        docket_tables = pd.read_html(content)\n",
    "\n",
    "        #error checking: gotta do this because there's different length of docket_list/\n",
    "        #usually docket texts are in docket_list[3], but not always\n",
    "        n = 0\n",
    "        while docket_tables[n].isin(['Docket Text']).sum().sum() == 0:\n",
    "            #print(n, docket_tables[n].isin(['Docket Text']).sum().sum())\n",
    "            n += 1\n",
    "                        \n",
    "        #print(i, files[i])\n",
    "        #print(docket_tables[n].head())\n",
    "\n",
    "        #docket_tables[n] is the docket text table\n",
    "        new_header = docket_tables[n].iloc[0]\n",
    "        docket_tables[n] = docket_tables[n][1:]\n",
    "        docket_tables[n].columns = new_header\n",
    "        \n",
    "        docket_tables[n]['#'] = pd.to_numeric(docket_tables[n]['#'],\n",
    "                                              downcast = 'signed', errors = 'coerce')\n",
    "        docket_tables[n]['Date Filed'] = pd.to_datetime(docket_tables[n]['Date Filed'])\n",
    "        docket_tables[n]['Case ID'] = case_id\n",
    "\n",
    "        df_docket_texts = pd.concat([df_docket_texts, docket_tables[n]])\n",
    "    #reorder a column\n",
    "    cols = list(df_docket_texts.columns)\n",
    "    df_docket_texts = df_docket_texts[[cols[-1]] + cols[:-1]]\n",
    "    \n",
    "    print('current docket text table size/shape: {}'.format(df_docket_texts.shape))\n",
    "    return df_docket_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current docket text table size/shape: (1804, 4)\n",
      "docket text 0\n",
      "FILING ERROR - DEFICIENT PLEADING - FILED AGAINST PARTY ERROR COMPLAINT against All Defendants. (Filing Fee $ 400.00, Receipt Number 0208-11793625)Document filed by Majid Soueidan.(Rosen, Samuel) Modified on 1/5/2016 (pc). (Entered: 01/04/2016) \n",
      "\n",
      "docket text 1\n",
      "FILING ERROR PDF ERROR CIVIL COVER SHEET filed. (Rosen, Samuel) Modified on 1/5/2016 (pc). (Entered: 01/04/2016) \n",
      "\n",
      "docket text 2\n",
      "REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-EASTERN CORPORATION, BRAD PEDERSEN, ROBERT J. KELLY, NELSON OBUS, WILLIAM M. SHOCKLEY, and SERGE DUPUIS, re: 1 Complaint. Document filed by Majid Soueidan. (Rosen, Samuel) (Entered: 01/04/2016) \n",
      "\n",
      "docket text 3\n",
      "RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Corporate Parent. Document filed by Majid Soueidan.(Rosen, Samuel) (Entered: 01/04/2016) \n",
      "\n",
      "docket text 4\n",
      "***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEADING. Notice to Attorney Samuel Kenneth Rosen to RE-FILE Document No. 1 Complaint,. The filing is deficient for the following reason(s): the All Defendant radio button was selected. Re-file the pleading using the event type Complaint found under the event list Complaints and Other Initiating Documents - attach the correct signed PDF - select the individually named filer/filers - select the individually named party/parties the pleading is against. (pc) (Entered: 01/05/2016) \n",
      "\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = grab_docket_test()\n",
    "docket_original = list(df['Docket Text'])\n",
    "for i in range(5):\n",
    "    print('docket text {}'.format(i))\n",
    "    print(docket_original[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_to_model = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\classifiers\\english.all.3class.distsim.crf.ser.gz'\n",
    "path_to_jar = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\stanford-ner.jar'\n",
    "tagger = StanfordNERTagger(path_to_model, path_to_jar = path_to_jar)\n",
    "\n",
    "output = []\n",
    "#length = 100 \n",
    "length = len(docket_original)\n",
    "for i in range(length):\n",
    "    org_str = []\n",
    "    name_str = []\n",
    "    stripped_str1 = []\n",
    "    stripped_str2 = []\n",
    "    tokens = nltk.tokenize.word_tokenize(docket_original[i])\n",
    "    for label in tagger.tag(tokens):\n",
    "        #print(label)\n",
    "        if label[1] == 'ORGANIZATION':\n",
    "            org_str.append(label[0])\n",
    "            stripped_str1.append('-ORG-')\n",
    "        elif label[1] == 'PERSON':\n",
    "            name_str.append(label[0])\n",
    "            stripped_str1.append('-NAME-')\n",
    "        else:\n",
    "            stripped_str1.append(label[0])\n",
    "            stripped_str2.append(label[0])\n",
    "    \n",
    "    output.append([docket_original[i],\n",
    "                   ' '.join(org_str),\n",
    "                   ' '.join(name_str),\n",
    "                   ' '.join(stripped_str1),\n",
    "                   ' '.join(stripped_str2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df = pd.DataFrame(output, columns = ['Original Docket Text', 'Organization Portion', 'Name Portion', \n",
    "                                         'Identifying Org and Name', 'Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = NER_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Original Docket Text Organization Portion  \\\n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...                        \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...                        \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...                        \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...                        \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...                        \n",
      "\n",
      "                                        Name Portion  \\\n",
      "0                        Majid Soueidan Rosen Samuel   \n",
      "1                                       Rosen Samuel   \n",
      "2  BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...   \n",
      "3                        Majid Soueidan Rosen Samuel   \n",
      "4                               Samuel Kenneth Rosen   \n",
      "\n",
      "                            Identifying Org and Name  \\\n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
      "\n",
      "                               Stripped Org and Name  \n",
      "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...  \n",
      "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...  \n",
      "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...  \n",
      "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...  \n",
      "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...  \n"
     ]
    }
   ],
   "source": [
    "print(new_df.head())\n",
    "docket_text_list = list(new_df['Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess1(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('(s)', 's')\n",
    "    text = text.replace(\"'s\", 's')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.replace('&', ' ')\n",
    "    return text\n",
    "\n",
    "def text_preprocess2(text):\n",
    "    text = text.replace('.', '')\n",
    "    return text\n",
    "\n",
    "def remove_stop(sentence):\n",
    "    output = []\n",
    "    for word in sentence.split():\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            output.append(word)\n",
    "    return ' '.join(output)\n",
    "\n",
    "keywords = pd.read_csv('docket_texts/keywords.csv', header = None)\n",
    "keywords.columns = ['keywords']\n",
    "keyword_list = list(keywords['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filing error pdf error civil cover sheet filed .  modified on 152016  . \n",
      "filing error pdf error civil cover sheet filed .  modified on 152016  . \n",
      "1804\n"
     ]
    }
   ],
   "source": [
    "print(docket_text_list[1])\n",
    "docket_text_list = [text_preprocess1(sentence).lower() for sentence in docket_text_list]\n",
    "print(docket_text_list[1])\n",
    "print(len(docket_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(remove_stop(sent)) for sent in sentences]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class LemmatizationWithPOSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_wordnet_pos(self,treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def pos_tag(self,tokens):\n",
    "        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "\n",
    "        # lemmatization using pos tagg   \n",
    "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
    "        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
    "        return pos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = Splitter()\n",
    "lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n",
    "\n",
    "lemma_docket_text_list = []\n",
    "for docket_text in docket_text_list:\n",
    "    #step 1 split document into sentence followed by tokenization\n",
    "    tokens = splitter.split(docket_text)\n",
    "\n",
    "    #step 2 lemmatization using pos tagger \n",
    "    lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "    lemma_docket_text_list.append(lemma_pos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804\n",
      "3\n",
      "10\n",
      "('filing', 'file', ['VBG'])\n",
      "filing\n"
     ]
    }
   ],
   "source": [
    "print(len(lemma_docket_text_list)) #docket text document level\n",
    "print(len(lemma_docket_text_list[0])) #docket text sentence level\n",
    "print(len(lemma_docket_text_list[0][0])) #docket text word level\n",
    "print(lemma_docket_text_list[0][0][0]) #docket text token level\n",
    "print(lemma_docket_text_list[0][0][0][0]) #docket text tuple level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do a collection of what we have\n",
    "collection = {}\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    for sentence in lemma_pos_token:\n",
    "        for token in sentence:\n",
    "            #print(token[2][0])\n",
    "            if token[2][0] not in list(collection.keys()):\n",
    "                collection[token[2][0]] = []\n",
    "                collection[token[2][0]].append(token[1])\n",
    "            else:\n",
    "                if token[1] not in collection[token[2][0]]:\n",
    "                    collection[token[2][0]].append(token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(dict([ (k, pd.Series(v)) for k, v in collection.items()])).to_csv('NLP_pos.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['file error deficient plead file error complaint .', 'document file .', '.'], ['file error pdf error civil cover sheet file .', '.'], ['request issuance summons breezeeastern corporation complaint .', 'document file .'], ['rule corporate disclosure .', 'corporate parent .', 'document file .'], ['notice deficient pleading .', 'notice refile document .', 'complaint .', 'file deficient reason radio button select .', 'refile plead use event type complaint find event list complaint initiate document correct pdf select individually name filerfilers select individually name partyparties plead .'], ['notice modification .', 'notice .', 'partyparties .', 'partyparties reasonreasons name contain typographical error text omit .'], ['notice deficient civil cover sheet .', 'notice refile document .', 'civil cover sheet .', 'file deficient reason citizenship principal fill despite federal question jurisdiction .'], ['electronic summons issue .'], ['notice open statistical error correction notice .', 'open statistical erroneously selectedentered fee status code due .', 'correction entry fee status code pd .'], ['open initial assignment notice aboveentitled assign .', 'please download assign locate http nysd.uscourts.govjudgesdistrict .', 'responsible courtesy individual practice .', 'please download rule instruction locate http nysd.uscourts.govecf_filing.php .']]\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "remove_pos = [\"``\", \"NNPS\", \"NNP\", \"CD\", '#', '$', \"''\", \",\", \"0\", \":\"]\n",
    "remove_word = [\"'s\", \"judge\", \"party\", \"defendant\", \"ex\", \"plantiff\", \"shall\", \"date\", \"b\", \"exhibit\", \"pennsylvania\", \"sign_judge\", \n",
    "               \"Inc..\", \"inc..\", \"llc\", \"'\", \"[_]\", \"action\", \"clerk\", \"july\", \"kw\", \"regard\", \"sac\", \"attachment\", \"c.d\", \"cal\", \"case\", \"cd\", \"l.p.\", \n",
    "               \"claim\", \"copy\", \"court\", \"direct\", \"form\", \"hereby\", \"magistrate\", \"p.c\", \"pl\", \"plaintiff\", \"regard\", \"sign\", \"time\", \"mr.\", \n",
    "               \"docket\", \"follow\", \"set\", \"matter\" \"agreement\" \"proceeding\", \"cotton\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \n",
    "               \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "               \"agreement\", \"v.\", \"place_vault\", \"modify\", \"fund\", \"associated\", \"provide\", \"material\", \"amount\", \"accordingly\", \"additional\", \n",
    "               \"second\", \"esq\", \"transmission\", \"g.c.\", \"seal\", \"review\", \"honor\", \"submit\", \"counsel\", \"witness\", \"civ\", \"first\", \"ltd..\", \"enter\", \n",
    "               \"stay\", \"forth\", \"matter\", \"whether\", \"class\", \"master\", \"information\", \"statement\", \"submission\", \"related\", \"see\", \"make\", \"paper\", \n",
    "               \"brookfield\", \"designate\", \"remain\", \"reportertranscriber\", \"submit\", \"include\", \"mail\", \"fact\", \"refer\", \"take\", \"pursuant\", \"amount\", \n",
    "               \"behalf\", \"I.p..\", \"must\", \"attorney\",\n",
    "               'abovecapitoned', 'attach', 'add', 'concern', 'chamber', 'close', 'district', 'damage', 'later', \n",
    "               'relate', 'return', 'require', 'restriction', 'respect', 'ny', 'seek', 'write', 'expert', 'transcript', \n",
    "               'day', 'h.o', 'damage', 'pre', 'proceeding', 'present', 'page', 'pending', 'p.m.', 'frcp', 'g.c.', 'record', 'r.']\n",
    "\n",
    "    \n",
    "#rebuild corpus\n",
    "docket_texts_output = [] #ultimate output after cleaning\n",
    "\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    docket_text_output = [] \n",
    "    for sentence in lemma_pos_token:\n",
    "        sentence_output = []\n",
    "        for token in sentence:\n",
    "            #print(token[1])\n",
    "            \n",
    "            if token[2][0] not in remove_pos: #if the pos is not in the remove_pos list\n",
    "                if token[1] not in remove_word: #these are the intentionally left out words\n",
    "                    sentence_output.append(token[1]) #append the the sentence\n",
    "        docket_text_output.append(' '.join(sentence_output))\n",
    "    docket_texts_output.append(docket_text_output)\n",
    "print(docket_texts_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>[file error deficient plead file error complai...</td>\n",
       "      <td>[file error_deficient plead file error complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td></td>\n",
       "      <td>Rosen Samuel</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>[file error pdf error civil cover sheet file ....</td>\n",
       "      <td>[file error pdf error civil_cover_sheet file ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td></td>\n",
       "      <td>BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>[request issuance summons breezeeastern corpor...</td>\n",
       "      <td>[request_issuance_summons breezeeastern corpor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>[rule corporate disclosure ., corporate parent...</td>\n",
       "      <td>[., ., document file .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td></td>\n",
       "      <td>Samuel Kenneth Rosen</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>[notice deficient pleading ., notice refile do...</td>\n",
       "      <td>[notice deficient pleading ., notice refile do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text Organization Portion  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...                        \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...                        \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...                        \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...                        \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...                        \n",
       "\n",
       "                                        Name Portion  \\\n",
       "0                        Majid Soueidan Rosen Samuel   \n",
       "1                                       Rosen Samuel   \n",
       "2  BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...   \n",
       "3                        Majid Soueidan Rosen Samuel   \n",
       "4                               Samuel Kenneth Rosen   \n",
       "\n",
       "                            Identifying Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                               Stripped Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \\\n",
       "0  [file error deficient plead file error complai...   \n",
       "1  [file error pdf error civil cover sheet file ....   \n",
       "2  [request issuance summons breezeeastern corpor...   \n",
       "3  [rule corporate disclosure ., corporate parent...   \n",
       "4  [notice deficient pleading ., notice refile do...   \n",
       "\n",
       "                          Apply Trigram Phrase Model  \n",
       "0  [file error_deficient plead file error complai...  \n",
       "1  [file error pdf error civil_cover_sheet file ....  \n",
       "2  [request_issuance_summons breezeeastern corpor...  \n",
       "3                            [., ., document file .]  \n",
       "4  [notice deficient pleading ., notice refile do...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Removed unnecessary POS & vocab'] = pd.Series(docket_texts_output)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_transform(texts):\n",
    "    trigram_output = []\n",
    "\n",
    "    remove_trigram = ['calendar_day', 'court_notice_intend', 'minute_entry_proceeding_hold', 'court_reportertranscriber_abovecaptioned_matter',\n",
    "                      'redaction_calendar_day', 'rule_statement', 'obtain_pacer', 'may_obtain_pacer', 'reportertranscriber_abovecaptioned_matter',\n",
    "                      'redact_transcript_deadline', 'send_chamber', \"official_transcript_notice_give\", \"notice_intent_request\", \"proceed_hold\", \n",
    "                      \"fee_receipt_number\", \"civil_procedure\", \"pursuant_frcp\", \"official_transcript_conference\", \n",
    "                      \"purchase_reportertranscriber_deadline_release\", \"et_al\", \"mail_chamber\", \"transcript_restriction\", \"redaction_transcript\", \n",
    "                      \"transcript_view_public_terminal\", \"transcript_make_remotely\", \"associated_et_al\", \"electronically_available_public_without\", \n",
    "                      \"genesys_id\", \"release_transcript_restriction\", \"adar_bay\", \"redaction_request_due\", \"new_york\", \"official_transcript_conference\", \n",
    "                      \"transcript_make_remotely\", \"transcript_proceeding_conference_hold\", \"redaction_transcript\",\n",
    "                      'affidavit_jr._c.p.a', 'corporate_parent', 'certain_underwriter', 'federal_rule_civil_procedure', 'redaction_request', \n",
    "                      'official_transcript', 'rule_disclosure', 'rule_corporate_disclosure', 'place_vault', 'public_without_redaction_calendar', \n",
    "                      'purchase_deadline_release_transcript', 'transcript_proceeding_hold', 'transcript_remotely_electronically_available']\n",
    "  \n",
    "\n",
    "    for sentence in texts:\n",
    "        unigram_review = []\n",
    "        for word in sentence.split():\n",
    "            unigram_review.append(word)\n",
    "    \n",
    "        #print('Uni: ', unigram_review)\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        #print('Bi: ', bigram_review)\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "        trigram_review = [phrase for phrase in trigram_review if phrase not in remove_trigram]\n",
    "        #print('Tri: ', trigram_review)\n",
    "        trigram_output.append(' '.join(trigram_review))\n",
    "    return trigram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['request_issuance_summons breezeeastern corporation complaint .',\n",
       " 'document file .']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model_filepath = 'docket_texts/bigram_model_noorgnoname'\n",
    "trigram_model_filepath = 'docket_texts/trigram_model_nonamenoorg'\n",
    "\n",
    "bigram_model = Phrases.load(bigram_model_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)\n",
    "\n",
    "new_df['Apply Trigram Phrase Model'] = new_df['Removed unnecessary POS & vocab'].apply(trigram_transform)\n",
    "new_df['Apply Trigram Phrase Model'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write trigram to file, not sure why...\n",
    "trigram_dockets_filepath = 'docket_texts/test/trigram_transformed_dockets_noorgnoname.txt'\n",
    "\n",
    "with codecs.open(trigram_dockets_filepath, 'w', encoding= 'utf_8') as f:\n",
    "    for i in range(len(new_df['Apply Trigram Phrase Model'])):\n",
    "        f.write(' '.join(new_df['Apply Trigram Phrase Model'][i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = 'docket_texts/test/trigram_dict_noorgnoname.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#some dictionary hyperparameters:\n",
    "no_below = 10 #reference is 10\n",
    "no_above = 0.4 #reference is 0.4\n",
    "\n",
    "trigram_reviews = LineSentence(trigram_dockets_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare otrigram_reviewsr too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below = no_below, no_above = no_above) #this step is questionable. May need to change the parameters\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "#trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = 'docket_texts/test/trigram_bow_corpus_noorgnoname.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        #print(review)\n",
    "        #print(trigram_dictionary.doc2bow(review))\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = 'docket_texts/test/trigram_sentences_nonamenoorg.txt'\n",
    "\n",
    "with codecs.open(trigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for trigram_sentence in list(new_df['Apply Trigram Phrase Model']):\n",
    "        #print(' '.join(trigram_sentence))\n",
    "        f.write(' '.join(trigram_sentence) + '\\n')\n",
    "        \n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = 'docket_texts/test/trigram_bow_corpus_noorgnoname.mm'\n",
    "trigram_dictionary_filepath = 'docket_texts/test/trigram_dict_noorgnoname.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(1804 documents, 319 features, 9389 non-zero entries)\n",
      "Wall time: 80.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# generate bag-of-words representations for\n",
    "# all reviews and save them as a matrix\n",
    "MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_sentences_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "print(trigram_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 76.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#some dictionary hyperparameters:\n",
    "no_below = 10 #reference is 10\n",
    "no_above = 0.4 #reference is 0.4\n",
    "\n",
    "trigram_reviews = LineSentence(trigram_dockets_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare otrigram_reviewsr too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below = no_below, no_above = no_above) #this step is questionable. May need to change the parameters\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaMulticore.load('docket_texts/train/lda_model_noorgnomodel_4')\n",
    "train_dict = Dictionary.load('docket_texts/train/trigram_dict_noorgnoname.dict')\n",
    "\n",
    "test_dict = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 pak6\n",
      "13 entry\n",
      "4 correct\n",
      "44 error_deficient_entry\n",
      "56 pro_hac_vice\n",
      "137 call\n",
      "143 setreset_deadline\n",
      "78 support\n",
      "73 oral_argument\n",
      "30 address\n",
      "180 without_prejudice\n",
      "171 reason_state\n",
      "132 complaint..\n",
      "12 due\n",
      "125 join\n",
      "10 refile\n",
      "163 approval\n",
      "271 cost\n",
      "160 memo_endorsement\n",
      "269 escrow\n",
      "281 reimbursement\n",
      "21 order\n",
      "155 either\n",
      "133 deem\n",
      "186 summons_issue\n",
      "151 compel_arbitration\n",
      "311 donee\n",
      "62 brief\n",
      "16 notice_appearance\n",
      "152 status_conference_hold\n",
      "314 administration\n",
      "233 j\n",
      "52 standing\n",
      "170 withdraw\n",
      "223 endorsed_letter_address\n",
      "39 adjourn_conference\n",
      "140 minute_entry_hold_telephone\n",
      "181 also\n",
      "305 asset\n",
      "273 petition\n",
      "302 temporary\n",
      "232 agent\n",
      "50 propose\n",
      "130 receive\n",
      "277 thereof\n",
      "95 transmit\n",
      "312 estate\n",
      "210 dated\n",
      "263 official\n",
      "279 expense\n",
      "254 law\n",
      "103 new\n",
      "41 adjourn\n",
      "61 serve_answer_due\n",
      "114 extend\n",
      "168 proceed\n",
      "248 associate\n",
      "204 office\n",
      "149 application_grant\n",
      "85 calendar\n",
      "182 hear\n",
      "23 rule\n",
      "90 without_redaction_calendar\n",
      "193 counterclaim\n",
      "141 nomorerack.com\n",
      "58 via\n",
      "32 joint\n",
      "131 amendcorrect\n",
      "9 notice\n",
      "303 testimony\n",
      "60 affidavit_service\n",
      "224 today\n",
      "144 amend\n",
      "53 within\n",
      "199 email\n",
      "209 respond\n",
      "285 period\n",
      "196 setreset_hearing\n",
      "100 answer\n",
      "215 york\n",
      "267 bench_trial\n",
      "251 llp\n",
      "179 application\n",
      "242 consolidated\n",
      "293 etc\n",
      "102 deficient_entry\n",
      "191 contact\n",
      "79 conference_hold\n",
      "104 note_refile\n",
      "194 third\n",
      "54 jr.\n",
      "120 consent\n",
      "22 respective\n",
      "138 p.m\n",
      "221 deposit\n",
      "20 herein\n",
      "123 estimate\n",
      "25 service\n",
      "275 tokyo\n",
      "42 letter_address\n",
      "57 support_staff..\n",
      "35 grant\n",
      "47 filing\n",
      "247 limine\n",
      "68 act\n",
      "70 security\n",
      "228 monday\n",
      "211 prejudice\n",
      "150 status_conference\n",
      "122 discovery\n",
      "255 necessary\n",
      "218 notify\n",
      "268 count\n",
      "116 trial\n",
      "230 supplemental\n",
      "165 appear\n",
      "128 without\n",
      "237 confirm\n",
      "238 fee_expense\n",
      "220 account\n",
      "33 letter\n",
      "188 initial_pretrial_conference\n",
      "236 person\n",
      "48 good\n",
      "297 verify\n",
      "306 property\n",
      "145 inc.\n",
      "259 joint_pretrial\n",
      "11 select\n",
      "274 preliminary_injunction\n",
      "126 mediator\n",
      "245 well\n",
      "256 alternatively\n",
      "19 deadline\n",
      "313 payment\n",
      "290 final_judgment\n",
      "212 pretrial\n",
      "185 l.l.c..\n",
      "257 argument\n",
      "154 settlement\n",
      "156 telephone\n",
      "98 section\n",
      "80 deadline_release\n",
      "36 hold\n",
      "92 reason\n",
      "110 extension\n",
      "225 decision\n",
      "244 vacate\n",
      "14 fee\n",
      "7 find_event_list\n",
      "316 verified\n",
      "226 ms.\n",
      "129 would\n",
      "203 default_judgment\n",
      "97 note\n",
      "109 courtroom\n",
      "298 disbursement\n",
      "34 premotion_conference\n",
      "40 schedule\n",
      "93 terminate\n",
      "239 proceeds\n",
      "178 upon\n",
      "176 relief\n",
      "71 declaration_support\n",
      "252 summary_judgment\n",
      "5 deficient\n",
      "307 receiver\n",
      "317 dismiss..\n",
      "219 place\n",
      "38 response\n",
      "81 redact_deadline\n",
      "207 plus\n",
      "167 prior\n",
      "227 violation\n",
      "147 amended_complaint\n",
      "205 certificate_service\n",
      "201 u.s\n",
      "69 litigation\n",
      "89 remotely_electronically_available_public\n",
      "115 jury\n",
      "124 indicate\n",
      "74 request\n",
      "200 represent\n",
      "153 assignment\n",
      "166 permit\n",
      "308 secure\n",
      "195 use\n",
      "187 move\n",
      "249 inc..filed\n",
      "243 right\n",
      "6 deficient_reason\n",
      "76 declaration_opposition\n",
      "289 reporter\n",
      "72 memorandum_law_support\n",
      "173 show_cause\n",
      "162 resolution\n",
      "3 request_issuance_summons\n",
      "190 management\n",
      "280 incur\n",
      "175 local_rule\n",
      "83 view_public_terminal_purchase\n",
      "91 opinion\n",
      "106 affidavit\n",
      "246 civil\n",
      "108 certify\n",
      "105 please\n",
      "65 minute_entry_hold\n",
      "43 appear_pro_hac_vice\n",
      "84 abovecaptioned\n",
      "51 select_correct\n",
      "301 transfer..\n",
      "270 approve\n",
      "164 event\n",
      "86 official_conference\n",
      "66 opposition\n",
      "15 status\n",
      "295 derivative\n",
      "288 memorandum_law\n",
      "1 error\n",
      "276 enjoin\n",
      "287 incorporate\n",
      "134 item\n",
      "183 purpose\n",
      "216 a.m.\n",
      "37 pm\n",
      "136 amend_complaint\n",
      "286 contempt\n",
      "107 centre_street_new_york\n",
      "96 number\n",
      "278 connection\n",
      "101 create\n",
      "59 deficiency\n",
      "88 redaction\n",
      "222 permission\n",
      "46 certificate\n",
      "161 reasonable\n",
      "202 deny\n",
      "235 nonparty\n",
      "250 friedman.filed\n",
      "8 name\n",
      "31 conference\n",
      "299 therefore\n",
      "146 list\n",
      "208 total\n",
      "264 certain\n",
      "177 state\n",
      "75 memorandum_law_opposition\n",
      "213 mediation\n",
      "300 transfer\n",
      "189 plan\n",
      "158 award\n",
      "28 new_address\n",
      "262 propose_finding_conclusion_law\n",
      "260 objection\n",
      "198 procedure\n",
      "296 shareholder\n",
      "184 endorsement_application_grant\n",
      "99 affiliate\n",
      "283 notice_appeal\n",
      "117 a.m\n",
      "121 deposition\n",
      "127 report\n",
      "284 consideration\n",
      "55 pay\n",
      "63 dismiss\n",
      "0 complaint\n",
      "294 amended\n",
      "169 error_electronic_filing_nonecf\n",
      "142 response_due\n",
      "197 default\n",
      "234 joinder\n",
      "174 hearing\n",
      "94 judgment\n",
      "17 accordance\n",
      "113 answer_due\n",
      "192 declaration\n",
      "318 xii\n",
      "139 telephone_conference\n",
      "26 stipulation\n",
      "24 serve\n",
      "49 issue\n",
      "18 agree\n",
      "172 sanction\n",
      "282 appeal\n",
      "272 arbitration\n",
      "310 appoint\n",
      "82 release\n",
      "118 complete\n",
      "291 restrain\n",
      "241 consolidate\n",
      "229 final\n",
      "159 endorsement\n",
      "111 fedex\n",
      "315 distribution\n",
      "265 memorandum\n",
      "148 service_accept\n",
      "309 receivership\n",
      "206 interest\n",
      "266 stipulate\n",
      "77 reply_memorandum_law\n",
      "253 kuczkir\n",
      "2 plead\n",
      "304 limine_preclude\n",
      "135 receipt\n",
      "67 reply\n",
      "217 effect\n",
      "29 notice_change_address\n",
      "240 otherwise\n",
      "292 sale\n",
      "64 leave\n",
      "261 currently\n",
      "258 authorize\n",
      "87 official_notice_give\n",
      "112 system\n",
      "119 conduct\n",
      "45 support_staff\n",
      "214 send\n",
      "27 undersigned\n",
      "157 advise\n"
     ]
    }
   ],
   "source": [
    "for new_id, token in test_dict.iteritems():\n",
    "    print(new_id, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0: 'motion to dismiss',\n",
    "               1: 'motion for summary judgment',\n",
    "               2: 'Complaint and motion',\n",
    "               3: 'Amended Complaint and motion'}\n",
    "\n",
    "topic_assignments = lda[trigram_bow_corpus]\n",
    "\n",
    "model_output = []\n",
    "\n",
    "for topic_assignment in topic_assignments:\n",
    "    model_output.append([(topic_names[topic[0]], topic[1]) for topic in sorted(topic_assignment, key = lambda x: x[1], reverse = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Topic Output'] = model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "      <th>Topic Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>FILING ERROR - DEFICIENT PLEADING - FILED AGAI...</td>\n",
       "      <td>[file error deficient plead file error complai...</td>\n",
       "      <td>[file error_deficient plead file error complai...</td>\n",
       "      <td>[(motion for summary judgment, 0.807523), (Ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td></td>\n",
       "      <td>Rosen Samuel</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>FILING ERROR PDF ERROR CIVIL COVER SHEET filed...</td>\n",
       "      <td>[file error pdf error civil cover sheet file ....</td>\n",
       "      <td>[file error pdf error civil_cover_sheet file ....</td>\n",
       "      <td>[(Amended Complaint and motion, 0.73956877), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td></td>\n",
       "      <td>BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...</td>\n",
       "      <td>[request issuance summons breezeeastern corpor...</td>\n",
       "      <td>[request_issuance_summons breezeeastern corpor...</td>\n",
       "      <td>[(motion for summary judgment, 0.74496585), (C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...</td>\n",
       "      <td></td>\n",
       "      <td>Majid Soueidan Rosen Samuel</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...</td>\n",
       "      <td>[rule corporate disclosure ., corporate parent...</td>\n",
       "      <td>[., ., document file .]</td>\n",
       "      <td>[(motion to dismiss, 0.25), (motion for summar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td></td>\n",
       "      <td>Samuel Kenneth Rosen</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...</td>\n",
       "      <td>[notice deficient pleading ., notice refile do...</td>\n",
       "      <td>[notice deficient pleading ., notice refile do...</td>\n",
       "      <td>[(motion to dismiss, 0.561045), (motion for su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text Organization Portion  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...                        \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...                        \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...                        \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Co...                        \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...                        \n",
       "\n",
       "                                        Name Portion  \\\n",
       "0                        Majid Soueidan Rosen Samuel   \n",
       "1                                       Rosen Samuel   \n",
       "2  BRAD PEDERSEN ROBERT J. KELLY NELSON OBUS WILL...   \n",
       "3                        Majid Soueidan Rosen Samuel   \n",
       "4                               Samuel Kenneth Rosen   \n",
       "\n",
       "                            Identifying Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                               Stripped Org and Name  \\\n",
       "0  FILING ERROR - DEFICIENT PLEADING - FILED AGAI...   \n",
       "1  FILING ERROR PDF ERROR CIVIL COVER SHEET filed...   \n",
       "2  REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-E...   \n",
       "3  RULE 7.1 CORPORATE DISCLOSURE STATEMENT . No C...   \n",
       "4  ***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEA...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \\\n",
       "0  [file error deficient plead file error complai...   \n",
       "1  [file error pdf error civil cover sheet file ....   \n",
       "2  [request issuance summons breezeeastern corpor...   \n",
       "3  [rule corporate disclosure ., corporate parent...   \n",
       "4  [notice deficient pleading ., notice refile do...   \n",
       "\n",
       "                          Apply Trigram Phrase Model  \\\n",
       "0  [file error_deficient plead file error complai...   \n",
       "1  [file error pdf error civil_cover_sheet file ....   \n",
       "2  [request_issuance_summons breezeeastern corpor...   \n",
       "3                            [., ., document file .]   \n",
       "4  [notice deficient pleading ., notice refile do...   \n",
       "\n",
       "                                        Topic Output  \n",
       "0  [(motion for summary judgment, 0.807523), (Ame...  \n",
       "1  [(Amended Complaint and motion, 0.73956877), (...  \n",
       "2  [(motion for summary judgment, 0.74496585), (C...  \n",
       "3  [(motion to dismiss, 0.25), (motion for summar...  \n",
       "4  [(motion to dismiss, 0.561045), (motion for su...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[['Original Docket Text','Apply Trigram Phrase Model', 'Topic Output']].to_csv('docket_texts/test/verify_this.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
