{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Goal:  \n",
    "Using existing NLP and LDA methodologies to perform topic modeling on docket texts. Three hyperparameters to consider:\n",
    "1. to remove organization or not in docket texts, so organizations themselves won't become topics.\n",
    "2. to remove names or not in docket texts, so names themselves won't become topics.\n",
    "3. variations in topic numbers: [2, 3, 5, 10]\n",
    "\n",
    "Will then perform visualizations and model summary output on every permutation/iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#visualization libraries\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = 'C:/Program Files/Java/jdk-10.0.1/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import itertools as it\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import corpus/docket texts from html to pandas DataFrame\n",
    "def grab_docket_test():\n",
    "    files = []\n",
    "    #get all .html files in the folder (all docket files are in .html)\n",
    "    for file in os.listdir('docket_texts/Train/'):\n",
    "        if file.endswith('.html'):\n",
    "            files.append(os.path.join('docket_texts/Train/', file))\n",
    "\n",
    "    df_docket_texts = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(files)): #gather all docket texts\n",
    "    #for i in [0, 1]: #for testing purposes\n",
    "        \n",
    "        content = codecs.open(files[i], 'r', 'utf-8').read()\n",
    "        #use beautiful soup to get the case ID\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        case_id = str(soup.find_all('h3'))    \n",
    "        bookmark1 = case_id.find('CASE #:') + len('CASE #:')\n",
    "        bookmark2 = case_id.find('</h3>')\n",
    "        case_id = case_id[bookmark1:bookmark2]\n",
    "\n",
    "        #use pandas to grab tables in the html files\n",
    "        docket_tables = pd.read_html(content)\n",
    "\n",
    "        #error checking: gotta do this because there's different length of docket_list/\n",
    "        #usually docket texts are in docket_list[3], but not always\n",
    "        n = 0\n",
    "        while docket_tables[n].isin(['Docket Text']).sum().sum() == 0:\n",
    "            #print(n, docket_tables[n].isin(['Docket Text']).sum().sum())\n",
    "            n += 1\n",
    "                        \n",
    "        #print(i, files[i])\n",
    "        #print(docket_tables[n].head())\n",
    "\n",
    "        #docket_tables[n] is the docket text table\n",
    "        new_header = docket_tables[n].iloc[0]\n",
    "        docket_tables[n] = docket_tables[n][1:]\n",
    "        docket_tables[n].columns = new_header\n",
    "        \n",
    "        docket_tables[n]['#'] = pd.to_numeric(docket_tables[n]['#'],\n",
    "                                              downcast = 'signed', errors = 'coerce')\n",
    "        docket_tables[n]['Date Filed'] = pd.to_datetime(docket_tables[n]['Date Filed'])\n",
    "        docket_tables[n]['Case ID'] = case_id\n",
    "\n",
    "        df_docket_texts = pd.concat([df_docket_texts, docket_tables[n]])\n",
    "    #reorder a column\n",
    "    cols = list(df_docket_texts.columns)\n",
    "    df_docket_texts = df_docket_texts[[cols[-1]] + cols[:-1]]\n",
    "    \n",
    "    print('current docket text table size/shape: {}'.format(df_docket_texts.shape))\n",
    "    return df_docket_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull from dir .html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current docket text table size/shape: (3244, 4)\n",
      "docket text 0\n",
      "COMPLAINT against Cardiogenics Holdings, Inc. filing fee $ 400, receipt number 0207-8445206 Was the Disclosure Statement on Civil Cover Sheet completed -YES,, filed by LG Capital Funding, LLC. (Steinmetz, Michael) (Additional attachment(s) added on 3/11/2016: # 1 Civil Cover Sheet, # 2 Proposed Summons) (Bowens, Priscilla). (Entered: 03/10/2016) \n",
      "\n",
      "docket text 1\n",
      "Case assigned to Judge Ann M Donnelly and Magistrate Judge Vera M. Scanlon. Please download and review the Individual Practices of the assigned Judges, located on our website. Attorneys are responsible for providing courtesy copies to judges where their Individual Practices require such. (Bowens, Priscilla) (Entered: 03/11/2016) \n",
      "\n",
      "docket text 2\n",
      "Summons Issued as to Cardiogenics Holdings, Inc.. (Bowens, Priscilla) (Entered: 03/11/2016) \n",
      "\n",
      "docket text 3\n",
      "NOTICE - emailed attorney regarding missing second page of the civil cover sheet. (Bowens, Priscilla) (Entered: 03/11/2016) \n",
      "\n",
      "docket text 4\n",
      "In accordance with Rule 73 of the Federal Rules of Civil Procedure and Local Rule 73.1, the parties are notified that if all parties consent a United States magistrate judge of this court is available to conduct all proceedings in this civil action including a (jury or nonjury) trial and to order the entry of a final judgment. Attached to the Notice is a blank copy of the consent form that should be filled out, signed and filed electronically only if all parties wish to consent. The form may also be accessed at the following link: http://www.uscourts.gov/uscourts/FormsAndFees/Forms/AO085.pdf. You may withhold your consent without adverse substantive consequences. Do NOT return or file the consent unless all parties have signed the consent. (Bowens, Priscilla) (Entered: 03/11/2016) \n",
      "\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = grab_docket_test()\n",
    "docket_original = list(df['Docket Text'])\n",
    "for i in range(5):\n",
    "    print('docket text {}'.format(i))\n",
    "    print(docket_original[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Stanford NER to identy Names and Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 31min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_to_model = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\classifiers\\english.all.3class.distsim.crf.ser.gz'\n",
    "path_to_jar = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\stanford-ner.jar'\n",
    "tagger = StanfordNERTagger(path_to_model, path_to_jar = path_to_jar)\n",
    "\n",
    "output = []\n",
    "#length = 10\n",
    "length = len(docket_original)\n",
    "for i in range(length):\n",
    "    org_str = []\n",
    "    name_str = []\n",
    "    stripped_str1 = []\n",
    "    stripped_str2 = []\n",
    "    tokens = nltk.tokenize.word_tokenize(docket_original[i])\n",
    "    #print(tokens)\n",
    "    for label, token in zip(tagger.tag(tokens), tokens):\n",
    "        #print(label)\n",
    "        if label[1] == 'ORGANIZATION':\n",
    "            org_str.append(label[0])\n",
    "            stripped_str1.append('-ORG-')\n",
    "        elif label[1] == 'PERSON':\n",
    "            name_str.append(label[0])\n",
    "            stripped_str1.append('-NAME-')\n",
    "        else:\n",
    "            stripped_str1.append(token)\n",
    "            stripped_str2.append(token)\n",
    "    \n",
    "    output.append([docket_original[i],\n",
    "                   ' '.join(org_str),\n",
    "                   ' '.join(name_str),\n",
    "                   ' '.join(stripped_str1),\n",
    "                   ' '.join(stripped_str2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df = pd.DataFrame(output, columns = ['Original Docket Text', 'Organization Portion', 'Name Portion', \n",
    "                                         'Identifying Org and Name', 'Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To re-build new_df, start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = NER_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before dedupe: (3244, 5)\n",
      "shape after dedupe: (3203, 5)\n"
     ]
    }
   ],
   "source": [
    "print('shape before dedupe: {}'.format(new_df.shape))\n",
    "new_df.drop_duplicates(inplace = True)\n",
    "print('shape after dedupe: {}'.format(new_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Original Docket Text  \\\n",
      "0  COMPLAINT against Cardiogenics Holdings, Inc. ...   \n",
      "1  Case assigned to Judge Ann M Donnelly and Magi...   \n",
      "2  Summons Issued as to Cardiogenics Holdings, In...   \n",
      "3  NOTICE - emailed attorney regarding missing se...   \n",
      "4  In accordance with Rule 73 of the Federal Rule...   \n",
      "\n",
      "                              Organization Portion  \\\n",
      "0  Cardiogenics Holdings , Inc. LG Capital Funding   \n",
      "1      Individual Practices of the assigned Judges   \n",
      "2                            Cardiogenics Holdings   \n",
      "3                                                    \n",
      "4                                                    \n",
      "\n",
      "                                      Name Portion  \\\n",
      "0             ( Steinmetz Michael Bowens Priscilla   \n",
      "1  Ann M Donnelly Vera M. Scanlon Bowens Priscilla   \n",
      "2                                 Bowens Priscilla   \n",
      "3                                 Bowens Priscilla   \n",
      "4                                 Bowens Priscilla   \n",
      "\n",
      "                            Identifying Org and Name  \\\n",
      "0  COMPLAINT against -ORG- -ORG- -ORG- -ORG- fili...   \n",
      "1  Case assigned to Judge -NAME- -NAME- -NAME- an...   \n",
      "2  Summons Issued as to -ORG- -ORG- , Inc.. ( -NA...   \n",
      "3  NOTICE - emailed attorney regarding missing se...   \n",
      "4  In accordance with Rule 73 of the Federal Rule...   \n",
      "\n",
      "                               Stripped Org and Name  \n",
      "0  COMPLAINT against filing fee $ 400 , receipt n...  \n",
      "1  Case assigned to Judge and Magistrate Judge . ...  \n",
      "2  Summons Issued as to , Inc.. ( , ) ( Entered :...  \n",
      "3  NOTICE - emailed attorney regarding missing se...  \n",
      "4  In accordance with Rule 73 of the Federal Rule...  \n"
     ]
    }
   ],
   "source": [
    "print(new_df.head())\n",
    "docket_text_list = list(new_df['Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess1(text):\n",
    "    text = text.replace('/', ' ')\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('(s)', 's')\n",
    "    text = text.replace(\"'s\", 's')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.replace('&', ' ')\n",
    "    \n",
    "    table = text.maketrans(string.punctuation, len(string.punctuation) * ' ')\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "def text_preprocess2(text):\n",
    "    text = text.replace('.', '')\n",
    "    return text\n",
    "\n",
    "def remove_stop(sentence):\n",
    "    output = []\n",
    "    for word in sentence.split():\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            output.append(word)\n",
    "    return ' '.join(output)\n",
    "\n",
    "keywords = pd.read_csv('docket_texts/keywords.csv', header = None)\n",
    "keywords.columns = ['keywords']\n",
    "keyword_list = list(keywords['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This attorney case opening filing has been checked for quality control . See the attachment for corrections that were made , if any . ( , ) ( Entered : 03/11/2016 ) \n",
      "\n",
      "this attorney case opening filing has been checked for quality control   see the attachment for corrections that were made   if any    \n",
      "\n",
      "length of docket text dataset: 3203\n"
     ]
    }
   ],
   "source": [
    "print(docket_text_list[5], '\\n')\n",
    "docket_text_list = [text_preprocess1(sentence).lower() for sentence in docket_text_list]\n",
    "docket_text_list = [text_preprocess2(sentence) for sentence in docket_text_list]\n",
    "print(docket_text_list[5])\n",
    "print('\\nlength of docket text dataset: {}'.format(len(docket_text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(remove_stop(sent)) for sent in sentences]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class LemmatizationWithPOSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_wordnet_pos(self,treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def pos_tag(self,tokens):\n",
    "        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "\n",
    "        # lemmatization using pos tagg   \n",
    "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
    "        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
    "        return pos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = Splitter()\n",
    "lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n",
    "\n",
    "lemma_docket_text_list = []\n",
    "for docket_text in docket_text_list:\n",
    "    #step 1 split document into sentence followed by tokenization\n",
    "    tokens = splitter.split(docket_text)\n",
    "\n",
    "    #step 2 lemmatization using pos tagger \n",
    "    lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "    lemma_docket_text_list.append(lemma_pos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3203\n",
      "1\n",
      "[[('accordance', 'accordance', ['NN']), ('rule', 'rule', ['NN']), ('73', '73', ['CD']), ('federal', 'federal', ['JJ']), ('rules', 'rule', ['NNS']), ('civil', 'civil', ['JJ']), ('procedure', 'procedure', ['NN']), ('local', 'local', ['JJ']), ('rule', 'rule', ['NN']), ('73', '73', ['CD']), ('1', '1', ['CD']), ('parties', 'party', ['NNS']), ('notified', 'notify', ['VBN']), ('parties', 'party', ['NNS']), ('consent', 'consent', ['NN']), ('united', 'united', ['JJ']), ('states', 'state', ['NNS']), ('magistrate', 'magistrate', ['VBP']), ('judge', 'judge', ['NN']), ('court', 'court', ['NN']), ('available', 'available', ['JJ']), ('conduct', 'conduct', ['NN']), ('proceedings', 'proceeding', ['NNS']), ('civil', 'civil', ['JJ']), ('action', 'action', ['NN']), ('including', 'include', ['VBG']), ('trial', 'trial', ['NN']), ('order', 'order', ['NN']), ('entry', 'entry', ['NN']), ('final', 'final', ['JJ']), ('judgment', 'judgment', ['NN']), ('attached', 'attach', ['VBN']), ('notice', 'notice', ['RB']), ('blank', 'blank', ['JJ']), ('copy', 'copy', ['NN']), ('consent', 'consent', ['NN']), ('form', 'form', ['NN']), ('filled', 'fill', ['VBN']), ('signed', 'sign', ['VBD']), ('filed', 'file', ['VBN']), ('electronically', 'electronically', ['RB']), ('parties', 'party', ['NNS']), ('wish', 'wish', ['JJ']), ('consent', 'consent', ['NN']), ('form', 'form', ['NN']), ('may', 'may', ['MD']), ('also', 'also', ['RB']), ('accessed', 'access', ['VBD']), ('following', 'follow', ['VBG']), ('link', 'link', ['NN']), ('http', 'http', ['NN']), ('www', 'www', ['NN']), ('uscourts', 'uscourts', ['NNS']), ('gov', 'gov', ['NN']), ('uscourts', 'uscourts', ['NNS']), ('formsandfees', 'formsandfees', ['NNS']), ('forms', 'form', ['NNS']), ('ao085', 'ao085', ['VBP']), ('pdf', 'pdf', ['NN']), ('may', 'may', ['MD']), ('withhold', 'withhold', ['VB']), ('consent', 'consent', ['NN']), ('without', 'without', ['IN']), ('adverse', 'adverse', ['JJ']), ('substantive', 'substantive', ['JJ']), ('consequences', 'consequence', ['NNS']), ('return', 'return', ['VBP']), ('file', 'file', ['JJ']), ('consent', 'consent', ['NN']), ('unless', 'unless', ['IN']), ('parties', 'party', ['NNS']), ('signed', 'sign', ['VBN']), ('consent', 'consent', ['NN'])]]\n",
      "27\n",
      "('complaint', 'complaint', ['NN'])\n",
      "complaint\n"
     ]
    }
   ],
   "source": [
    "print(len(lemma_docket_text_list)) #docket text document level\n",
    "print(len(lemma_docket_text_list[4])) #docket text sentence level\n",
    "print(lemma_docket_text_list[4]) #docket text sentence level\n",
    "print(len(lemma_docket_text_list[0][0])) #docket text word level\n",
    "print(lemma_docket_text_list[0][0][0]) #docket text token level\n",
    "print(lemma_docket_text_list[0][0][0][0]) #docket text tuple level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do a collection of what we have, this is to see for seeing what we have from a POS perspective, and potentially remove those POS\n",
    "\n",
    "pos_collection = {}\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    for sentence in lemma_pos_token:\n",
    "        for token in sentence:\n",
    "            #print(token[2][0])\n",
    "            if token[2][0] not in list(pos_collection.keys()):\n",
    "                pos_collection[token[2][0]] = []\n",
    "                pos_collection[token[2][0]].append(token[1])\n",
    "            else:\n",
    "                if token[1] not in pos_collection[token[2][0]]:\n",
    "                    pos_collection[token[2][0]].append(token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict([ (k, pd.Series(v)) for k, v in collection.items()])).to_csv('NLP_train_pos.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pos = list(pd.read_excel('NLP_to_be_removed.xlsx', sheetname = 0, header = None)[0])\n",
    "remove_word = list(pd.read_excel('NLP_to_be_removed.xlsx', sheetname = 1, header = None)[0])\n",
    "remove_trigram = list(pd.read_excel('NLP_to_be_removed.xlsx', sheetname = 2, header = None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complaint fee receipt number disclosure civil cover sheet complete yes civil cover sheet propose summons', 'please download locate website responsible courtesy individual practice', 'summons inc', 'notice email miss civil cover sheet', 'accordance rule federal rule civil procedure local rule notify consent united available conduct civil trial order entry final judgment notice blank consent fill electronically wish consent also access link http www uscourts gov uscourts formsandfees ao085 pdf withhold consent without adverse substantive consequence consent unless consent', 'open check quality control correction', 'notice appearance', 'backend note complaint', 'ta letter complaint', 'c notice conversion complaint']\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "remove_pos = [\"``\", \"NNPS\", \"NNP\", \"CD\", '#', '$', \"''\", \",\", \"0\", \":\"]\n",
    "remove_word = [\"'s\", \"judge\", \"party\", \"defendant\", \"ex\", \"plantiff\", \"shall\", \"date\", \"b\", \"exhibit\", \"pennsylvania\",  \n",
    "               \"Inc..\", \"inc..\", \"llc\", \"'\", \"[_]\", \"action\", \"clerk\", \"july\", \"kw\", \"regard\", \"sac\", \"attachment\", \"c.d\", \"cal\", \"case\", \"cd\", \"l.p.\", \n",
    "               \"claim\", \"copy\", \"court\", \"direct\", \"form\", \"hereby\", \"magistrate\", \"p.c\", \"pl\", \"plaintiff\", \"regard\", \"sign\", \"time\", \"mr.\", \n",
    "               \"docket\", \"follow\", \"set\", \"matter\" \"agreement\" \"proceeding\", \"cotton\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \n",
    "               \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "               \"agreement\", \"v.\", \"modify\", \"fund\", \"associated\", \"provide\", \"material\", \"amount\", \"accordingly\", \"additional\", \n",
    "               \"second\", \"esq\", \"transmission\", \"g.c.\", \"seal\", \"review\", \"honor\", \"submit\", \"counsel\", \"witness\", \"civ\", \"first\", \"ltd..\", \"enter\", \n",
    "               \"stay\", \"forth\", \"matter\", \"whether\", \"class\", \"master\", \"information\", \"statement\", \"submission\", \"related\", \"see\", \"make\", \"paper\", \n",
    "               \"brookfield\", \"designate\", \"remain\", \"reportertranscriber\", \"submit\", \"include\", \"mail\", \"fact\", \"refer\", \"take\", \"pursuant\", \"amount\", \n",
    "               \"behalf\", \"I.p..\", \"must\", \"attorney\",\n",
    "               'abovecapitoned', 'attach', 'add', 'concern', 'chamber', 'close', 'district', 'damage', 'later', \n",
    "               'relate', 'return', 'require', 'restriction', 'respect', 'ny', 'seek', 'write', 'expert', 'transcript', \n",
    "               'day', 'h.o', 'damage', 'pre', 'proceeding', 'present', 'page', 'pending', 'p.m.', 'frcp', 'g.c.', 'record', 'r.',\n",
    "              'application', 'filing', 'issue', 'assign', 'iii', 'state', 'protocol', 'loan', 'error', 'file', 'document']\n",
    "'''\n",
    "    \n",
    "#rebuild corpus\n",
    "docket_texts_output = [] #ultimate output after cleaning\n",
    "\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    docket_text_output = ''\n",
    "    for sentence in lemma_pos_token:\n",
    "        sentence_output = []\n",
    "        for token in sentence:\n",
    "            #print(token[1])\n",
    "            \n",
    "            if token[2][0] not in remove_pos: #if the pos is not in the remove_pos list\n",
    "                if token[1] not in remove_word: #these are the intentionally left out words\n",
    "                    sentence_output.append(token[1]) #append the the sentence\n",
    "        docket_text_output += ' '.join(sentence_output)\n",
    "    docket_texts_output.append(docket_text_output)\n",
    "print(docket_texts_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPLAINT against Cardiogenics Holdings, Inc. ...</td>\n",
       "      <td>Cardiogenics Holdings , Inc. LG Capital Funding</td>\n",
       "      <td>( Steinmetz Michael Bowens Priscilla</td>\n",
       "      <td>COMPLAINT against -ORG- -ORG- -ORG- -ORG- fili...</td>\n",
       "      <td>COMPLAINT against filing fee $ 400 , receipt n...</td>\n",
       "      <td>complaint fee receipt number disclosure civil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case assigned to Judge Ann M Donnelly and Magi...</td>\n",
       "      <td>Individual Practices of the assigned Judges</td>\n",
       "      <td>Ann M Donnelly Vera M. Scanlon Bowens Priscilla</td>\n",
       "      <td>Case assigned to Judge -NAME- -NAME- -NAME- an...</td>\n",
       "      <td>Case assigned to Judge and Magistrate Judge . ...</td>\n",
       "      <td>please download locate website responsible cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summons Issued as to Cardiogenics Holdings, In...</td>\n",
       "      <td>Cardiogenics Holdings</td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>Summons Issued as to -ORG- -ORG- , Inc.. ( -NA...</td>\n",
       "      <td>Summons Issued as to , Inc.. ( , ) ( Entered :...</td>\n",
       "      <td>summons inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td></td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td>notice email miss civil cover sheet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td></td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td>accordance rule federal rule civil procedure l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text  \\\n",
       "0  COMPLAINT against Cardiogenics Holdings, Inc. ...   \n",
       "1  Case assigned to Judge Ann M Donnelly and Magi...   \n",
       "2  Summons Issued as to Cardiogenics Holdings, In...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                              Organization Portion  \\\n",
       "0  Cardiogenics Holdings , Inc. LG Capital Funding   \n",
       "1      Individual Practices of the assigned Judges   \n",
       "2                            Cardiogenics Holdings   \n",
       "3                                                    \n",
       "4                                                    \n",
       "\n",
       "                                      Name Portion  \\\n",
       "0             ( Steinmetz Michael Bowens Priscilla   \n",
       "1  Ann M Donnelly Vera M. Scanlon Bowens Priscilla   \n",
       "2                                 Bowens Priscilla   \n",
       "3                                 Bowens Priscilla   \n",
       "4                                 Bowens Priscilla   \n",
       "\n",
       "                            Identifying Org and Name  \\\n",
       "0  COMPLAINT against -ORG- -ORG- -ORG- -ORG- fili...   \n",
       "1  Case assigned to Judge -NAME- -NAME- -NAME- an...   \n",
       "2  Summons Issued as to -ORG- -ORG- , Inc.. ( -NA...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                               Stripped Org and Name  \\\n",
       "0  COMPLAINT against filing fee $ 400 , receipt n...   \n",
       "1  Case assigned to Judge and Magistrate Judge . ...   \n",
       "2  Summons Issued as to , Inc.. ( , ) ( Entered :...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \n",
       "0  complaint fee receipt number disclosure civil ...  \n",
       "1  please download locate website responsible cou...  \n",
       "2                                        summons inc  \n",
       "3                notice email miss civil cover sheet  \n",
       "4  accordance rule federal rule civil procedure l...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Removed unnecessary POS & vocab'] = pd.Series(docket_texts_output)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree to identify keywords and topics based on Chris' feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_topics_df = pd.read_csv('mannual_topics.csv')\n",
    "manual_topics_df = manual_topics_df.apply(lambda x: x.astype(str).str.lower())\n",
    "manual_topics_dict = manual_topics_df.to_dict('list')\n",
    "for topic in manual_topics_dict.keys():\n",
    "    manual_topics_dict[topic] = [keyword for keyword in manual_topics_dict[topic] if keyword != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mannual_topic_assignment(text):\n",
    "    text = text.split()\n",
    "    #print(text)\n",
    "    output = []\n",
    "    for topic in manual_topics_dict.keys():\n",
    "        if set(text).intersection(manual_topics_dict[topic]):\n",
    "            output.append(topic)\n",
    "    #print(output)\n",
    "    return ', '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_texts_output_DT = []\n",
    "topics_DT = []\n",
    "\n",
    "for text in docket_texts_output:\n",
    "    topic = mannual_topic_assignment(text)\n",
    "    #print(topic)\n",
    "    if topic != '':\n",
    "        docket_texts_output_DT.append('')\n",
    "        topics_DT.append(topic)\n",
    "    else:\n",
    "        docket_texts_output_DT.append(text)\n",
    "        topics_DT.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Complaints, Service of Process', '', 'Service of Process', 'Notices', 'Notices, Motions']\n",
      "['', 'please download locate website responsible courtesy individual practice', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(topics_DT[:5])\n",
    "print(docket_texts_output_DT[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPLAINT against Cardiogenics Holdings, Inc. ...</td>\n",
       "      <td>Cardiogenics Holdings , Inc. LG Capital Funding</td>\n",
       "      <td>( Steinmetz Michael Bowens Priscilla</td>\n",
       "      <td>COMPLAINT against -ORG- -ORG- -ORG- -ORG- fili...</td>\n",
       "      <td>COMPLAINT against filing fee $ 400 , receipt n...</td>\n",
       "      <td>complaint fee receipt number disclosure civil ...</td>\n",
       "      <td>Complaints, Service of Process</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case assigned to Judge Ann M Donnelly and Magi...</td>\n",
       "      <td>Individual Practices of the assigned Judges</td>\n",
       "      <td>Ann M Donnelly Vera M. Scanlon Bowens Priscilla</td>\n",
       "      <td>Case assigned to Judge -NAME- -NAME- -NAME- an...</td>\n",
       "      <td>Case assigned to Judge and Magistrate Judge . ...</td>\n",
       "      <td>please download locate website responsible cou...</td>\n",
       "      <td></td>\n",
       "      <td>please download locate website responsible cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summons Issued as to Cardiogenics Holdings, In...</td>\n",
       "      <td>Cardiogenics Holdings</td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>Summons Issued as to -ORG- -ORG- , Inc.. ( -NA...</td>\n",
       "      <td>Summons Issued as to , Inc.. ( , ) ( Entered :...</td>\n",
       "      <td>summons inc</td>\n",
       "      <td>Service of Process</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td></td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td>notice email miss civil cover sheet</td>\n",
       "      <td>Notices</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td></td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td>accordance rule federal rule civil procedure l...</td>\n",
       "      <td>Notices, Motions</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text  \\\n",
       "0  COMPLAINT against Cardiogenics Holdings, Inc. ...   \n",
       "1  Case assigned to Judge Ann M Donnelly and Magi...   \n",
       "2  Summons Issued as to Cardiogenics Holdings, In...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                              Organization Portion  \\\n",
       "0  Cardiogenics Holdings , Inc. LG Capital Funding   \n",
       "1      Individual Practices of the assigned Judges   \n",
       "2                            Cardiogenics Holdings   \n",
       "3                                                    \n",
       "4                                                    \n",
       "\n",
       "                                      Name Portion  \\\n",
       "0             ( Steinmetz Michael Bowens Priscilla   \n",
       "1  Ann M Donnelly Vera M. Scanlon Bowens Priscilla   \n",
       "2                                 Bowens Priscilla   \n",
       "3                                 Bowens Priscilla   \n",
       "4                                 Bowens Priscilla   \n",
       "\n",
       "                            Identifying Org and Name  \\\n",
       "0  COMPLAINT against -ORG- -ORG- -ORG- -ORG- fili...   \n",
       "1  Case assigned to Judge -NAME- -NAME- -NAME- an...   \n",
       "2  Summons Issued as to -ORG- -ORG- , Inc.. ( -NA...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                               Stripped Org and Name  \\\n",
       "0  COMPLAINT against filing fee $ 400 , receipt n...   \n",
       "1  Case assigned to Judge and Magistrate Judge . ...   \n",
       "2  Summons Issued as to , Inc.. ( , ) ( Entered :...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \\\n",
       "0  complaint fee receipt number disclosure civil ...   \n",
       "1  please download locate website responsible cou...   \n",
       "2                                        summons inc   \n",
       "3                notice email miss civil cover sheet   \n",
       "4  accordance rule federal rule civil procedure l...   \n",
       "\n",
       "                        DT Topics  \\\n",
       "0  Complaints, Service of Process   \n",
       "1                                   \n",
       "2              Service of Process   \n",
       "3                         Notices   \n",
       "4                Notices, Motions   \n",
       "\n",
       "                  Removed unnecessary POS & vocab DT  \n",
       "0                                                     \n",
       "1  please download locate website responsible cou...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['DT Topics'] = pd.Series(topics_DT)\n",
    "new_df['Removed unnecessary POS & vocab DT'] = pd.Series(docket_texts_output_DT)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Complaints, Service of Process\n",
      "complaint fee receipt number disclosure civil cover sheet complete yes civil cover sheet propose summons\n",
      "\n",
      "1\n",
      "\n",
      "please download locate website responsible courtesy individual practice\n",
      "please download locate website responsible courtesy individual practice\n",
      "2\n",
      "Service of Process\n",
      "summons inc\n",
      "\n",
      "3\n",
      "Notices\n",
      "notice email miss civil cover sheet\n",
      "\n",
      "4\n",
      "Notices, Motions\n",
      "accordance rule federal rule civil procedure local rule notify consent united available conduct civil trial order entry final judgment notice blank consent fill electronically wish consent also access link http www uscourts gov uscourts formsandfees ao085 pdf withhold consent without adverse substantive consequence consent unless consent\n",
      "\n",
      "5\n",
      "\n",
      "open check quality control correction\n",
      "open check quality control correction\n",
      "6\n",
      "Notices\n",
      "notice appearance\n",
      "\n",
      "7\n",
      "Complaints\n",
      "backend note complaint\n",
      "\n",
      "8\n",
      "Complaints\n",
      "ta letter complaint\n",
      "\n",
      "9\n",
      "Notices, Complaints\n",
      "c notice conversion complaint\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print some examples\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    if new_df['DT Topics'].iloc[i] != []:\n",
    "        print(new_df['DT Topics'].iloc[i])\n",
    "        print(new_df['Removed unnecessary POS & vocab'].iloc[i])\n",
    "        print(new_df['Removed unnecessary POS & vocab DT'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = 'docket_texts/train/DT/unigram_nltk_noorgnoname.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# turn the lemmatized corpus into unigram sentences\n",
    "with codecs.open(unigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for sentence in docket_texts_output_DT:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "0    complaint fee receipt number disclosure civil ...\n",
      "1    please download locate website responsible cou...\n",
      "2                                          summons inc\n",
      "3                  notice email miss civil cover sheet\n",
      "4    accordance rule federal rule civil procedure l...\n",
      "5                open check quality control correction\n",
      "6                                    notice appearance\n",
      "7                               backend note complaint\n",
      "8                                  ta letter complaint\n",
      "9                        c notice conversion complaint\n",
      "Name: Removed unnecessary POS & vocab, dtype: object\n",
      "0                                                     \n",
      "1    please download locate website responsible cou...\n",
      "2                                                     \n",
      "3                                                     \n",
      "4                                                     \n",
      "5                open check quality control correction\n",
      "6                                                     \n",
      "7                                                     \n",
      "8                                                     \n",
      "9                                                     \n",
      "Name: Removed unnecessary POS & vocab DT, dtype: object\n",
      "\n",
      "Unigram_sentence:\n",
      "please download locate website responsible courtesy individual practice\n",
      "\n",
      "open check quality control correction\n",
      "\n",
      "propose schedule order\n",
      "\n",
      "letter propose schedule order\n",
      "\n",
      "order respond letter order\n",
      "\n",
      "order respond letter motion order\n",
      "\n",
      "letter status\n",
      "\n",
      "reassign longer please download locate website responsible courtesy individual practice\n",
      "\n",
      "status report\n",
      "\n",
      "letter order\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's do some comparision between the original text and unigram sentences, shouldn't be that different.\n",
    "print('Original text:')\n",
    "print(new_df['Removed unnecessary POS & vocab'].iloc[:10])\n",
    "print(new_df['Removed unnecessary POS & vocab DT'].iloc[:10])\n",
    "#print(df['Docket Text'].iloc[1])\n",
    "\n",
    "print('\\nUnigram_sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, 0, 10):\n",
    "    print(' '.join(unigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = 'docket_texts/train/DT/bigram_model_noorgnoname' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# store our bigram model\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk if we don't want to run this again\n",
    "#bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = 'docket_texts/train/DT/bigram_sentences_noorgnoname.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.9 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# apply the bigram model, and write it to file\n",
    "with codecs.open(bigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram length = 552, bigram length = 552\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "print('unigram length = {}, bigram length = {}'.format(len(list(unigram_sentences)), len(list(bigram_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "complaint fee receipt number disclosure civil cover sheet complete yes civil cover sheet propose summons\n",
      "please download locate website responsible courtesy individual practice\n",
      "\n",
      "Unigram sentence:\n",
      "please download locate website responsible courtesy individual practice\n",
      "open check quality control correction\n",
      "propose schedule order\n",
      "letter propose schedule order\n",
      "order respond letter order\n",
      "order respond letter motion order\n",
      "letter status\n",
      "reassign longer please download locate website responsible courtesy individual practice\n",
      "status report\n",
      "letter order\n",
      "\n",
      "Bigram sentence:\n",
      "please_download locate_website responsible_courtesy individual_practice\n",
      "open_check quality_control correction\n",
      "propose schedule order\n",
      "letter propose schedule order\n",
      "order respond letter order\n",
      "order respond letter motion order\n",
      "letter status\n",
      "reassign longer please_download locate_website responsible_courtesy individual_practice\n",
      "status_report\n",
      "letter order\n"
     ]
    }
   ],
   "source": [
    "#original v. unigram v. bigram. Some phrases should be combined already\n",
    "start = 0\n",
    "finish = 10\n",
    "print('Original text:')\n",
    "print(new_df['Removed unnecessary POS & vocab'].iloc[0])\n",
    "print(new_df['Removed unnecessary POS & vocab'].iloc[1])\n",
    "\n",
    "print('\\nUnigram sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, 0, 10):\n",
    "    print(' '.join(unigram_sentence))\n",
    "print('\\nBigram sentence:')\n",
    "for bigram_sentence in it.islice(bigram_sentences, start, finish):\n",
    "    print(' '.join(bigram_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = 'docket_texts/train/DT/trigram_model_nonamenoorg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# again, using Phrases to attach more words to phrases already formed\n",
    "trigram_model = Phrases(bigram_sentences)\n",
    "trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "# load the finished model from disk\n",
    "#trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = 'docket_texts/train/DT/trigram_sentences_nonamenoorg.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.9 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open(trigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "        #print('Bi', bigram_sentence)\n",
    "        trigram_sentence = ' '.join(trigram_model[bigram_sentence])\n",
    "        #print('Tri', trigram_sentence)\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "complaint fee receipt number disclosure civil cover sheet complete yes civil cover sheet propose summons \n",
      "\n",
      "please download locate website responsible courtesy individual practice \n",
      "\n",
      "summons inc \n",
      "\n",
      "notice email miss civil cover sheet \n",
      "\n",
      "\n",
      "UNIGRAM Sentence:\n",
      "please download locate website responsible courtesy individual practice\n",
      "open check quality control correction\n",
      "propose schedule order\n",
      "letter propose schedule order\n",
      "order respond letter order\n",
      "order respond letter motion order\n",
      "letter status\n",
      "reassign longer please download locate website responsible courtesy individual practice\n",
      "status report\n",
      "letter order\n",
      "letter\n",
      "undeliverable decision order sender postal notation move unknown\n",
      "please download locate website responsible courtesy individual practice\n",
      "open check quality control correction\n",
      "schedule order hear order show cause reschedule courtroom south deadline effect order\n",
      "\n",
      "BIGRAM Sentence:\n",
      "please_download locate_website responsible_courtesy individual_practice\n",
      "open_check quality_control correction\n",
      "propose schedule order\n",
      "letter propose schedule order\n",
      "order respond letter order\n",
      "order respond letter motion order\n",
      "letter status\n",
      "reassign longer please_download locate_website responsible_courtesy individual_practice\n",
      "status_report\n",
      "letter order\n",
      "letter\n",
      "undeliverable decision order sender postal notation move unknown\n",
      "please_download locate_website responsible_courtesy individual_practice\n",
      "open_check quality_control correction\n",
      "schedule order hear order show_cause reschedule courtroom south deadline effect order\n",
      "\n",
      "TRIGRAM Sentence:\n",
      "please_download_locate_website responsible_courtesy_individual_practice\n",
      "open_check_quality_control correction\n",
      "propose schedule order\n",
      "letter propose schedule order\n",
      "order respond letter order\n",
      "order respond letter motion order\n",
      "letter status\n",
      "reassign longer please_download_locate_website responsible_courtesy_individual_practice\n",
      "status_report\n",
      "letter order\n",
      "letter\n",
      "undeliverable decision order sender postal notation move unknown\n",
      "please_download_locate_website responsible_courtesy_individual_practice\n",
      "open_check_quality_control correction\n",
      "schedule order hear order show_cause reschedule courtroom south deadline effect order\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "finish = 15\n",
    "print('Original text:')\n",
    "print(new_df['Removed unnecessary POS & vocab'].iloc[0],'\\n')\n",
    "print(new_df['Removed unnecessary POS & vocab'].iloc[1],'\\n')\n",
    "print(new_df['Removed unnecessary POS & vocab'].iloc[2],'\\n')\n",
    "print(new_df['Removed unnecessary POS & vocab'].iloc[3],'\\n')\n",
    "\n",
    "print('\\nUNIGRAM Sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, start, finish):\n",
    "    print(' '.join(unigram_sentence))\n",
    "print('\\nBIGRAM Sentence:')\n",
    "for bigram_sentence in it.islice(bigram_sentences, start, finish):\n",
    "    print(' '.join(bigram_sentence))\n",
    "print('\\nTRIGRAM Sentence:')\n",
    "for trigram_sentence in it.islice(trigram_sentences, start, finish):\n",
    "    print(' '.join(trigram_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_transform(texts):\n",
    "    display = False\n",
    "    texts = str(texts)\n",
    "    trigram_output = ''\n",
    "    #print(texts)\n",
    "    '''\n",
    "    remove_trigram = ['calendar_day', 'court_notice_intend', 'minute_entry_proceeding_hold', 'court_reportertranscriber_abovecaptioned_matter',\n",
    "                      'redaction_calendar_day', 'rule_statement', 'obtain_pacer', 'may_obtain_pacer', 'reportertranscriber_abovecaptioned_matter',\n",
    "                      'redact_transcript_deadline', 'send_chamber', \"official_transcript_notice_give\", \"notice_intent_request\", \"proceed_hold\", \n",
    "                      \"fee_receipt_number\", \"civil_procedure\", \"pursuant_frcp\", \"official_transcript_conference\", \n",
    "                      \"purchase_reportertranscriber_deadline_release\", \"et_al\", \"mail_chamber\", \"transcript_restriction\", \"redaction_transcript\", \n",
    "                      \"transcript_view_public_terminal\", \"transcript_make_remotely\", \"associated_et_al\", \"electronically_available_public_without\", \n",
    "                      \"genesys_id\", \"release_transcript_restriction\", \"adar_bay\", \"redaction_request_due\", \"new_york\", \"official_transcript_conference\", \n",
    "                      \"transcript_make_remotely\", \"transcript_proceeding_conference_hold\", \"redaction_transcript\",\n",
    "                      'affidavit_jr._c.p.a', 'corporate_parent', 'certain_underwriter', 'federal_rule_civil_procedure', 'redaction_request', \n",
    "                      'official_transcript', 'rule_disclosure', 'rule_corporate_disclosure', 'place_vault', 'public_without_redaction_calendar', \n",
    "                      'purchase_deadline_release_transcript', 'transcript_proceeding_hold', 'transcript_remotely_electronically_available',\n",
    "                      'minute_entry_hold', 'discovery_hear_hold', 'jury_trial_hold', \"sign_judge\",'place_vault']\n",
    "    '''\n",
    "    if texts == None:\n",
    "        return None\n",
    "    \n",
    "    unigram_review = []\n",
    "    for word in texts.split():\n",
    "        unigram_review.append(word)\n",
    "    if display:\n",
    "        print('Uni: ', unigram_review)\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    if display:\n",
    "        print('Bi: ', bigram_review)\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    if display:\n",
    "        print('Tri: ', trigram_review)\n",
    "    trigram_review = [phrase for phrase in trigram_review if phrase not in remove_trigram]\n",
    "    if display:\n",
    "        print('Tri removed: ', trigram_review)\n",
    "    trigram_output += ' '.join(trigram_review)\n",
    "    \n",
    "    return trigram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "new_df['Apply Trigram Phrase Model'] = new_df['Removed unnecessary POS & vocab DT'].apply(trigram_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Complaints, Service of Process</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>please_download_locate_website responsible_cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Service of Process</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notices</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Notices, Motions</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        DT Topics  \\\n",
       "0  Complaints, Service of Process   \n",
       "1                                   \n",
       "2              Service of Process   \n",
       "3                         Notices   \n",
       "4                Notices, Motions   \n",
       "\n",
       "                          Apply Trigram Phrase Model  \n",
       "0                                                     \n",
       "1  please_download_locate_website responsible_cou...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['DT Topics','Apply Trigram Phrase Model']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write trigram to file\n",
    "trigram_dockets_filepath = 'docket_texts/train/DT/trigram_transformed_dockets_noorgnoname.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(trigram_dockets_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for i in range(len(new_df['Apply Trigram Phrase Model'])):\n",
    "        text = new_df['Apply Trigram Phrase Model'].iloc[i]\n",
    "        if text != '':\n",
    "            #print(text)\n",
    "            f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = 'docket_texts/train/DT/trigram_dict_noorgnoname.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#some dictionary hyperparameters:\n",
    "no_below = 10 #reference is 10\n",
    "no_above = 0.4 #reference is 0.4\n",
    "\n",
    "trigram_reviews = LineSentence(trigram_dockets_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare otrigram_reviewsr too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below = no_below, no_above = no_above) #this step is questionable. May need to change the parameters\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "#trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = 'docket_texts/train/DT/trigram_bow_corpus_noorgnoname.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        #print(review)\n",
    "        #print(trigram_dictionary.doc2bow(review))\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(552 documents, 32 features, 832 non-zero entries)\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# generate bag-of-words representations for\n",
    "# all reviews and save them as a matrix\n",
    "MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_sentences_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "print(trigram_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPLAINT against Cardiogenics Holdings, Inc. ...</td>\n",
       "      <td>Cardiogenics Holdings , Inc. LG Capital Funding</td>\n",
       "      <td>( Steinmetz Michael Bowens Priscilla</td>\n",
       "      <td>COMPLAINT against -ORG- -ORG- -ORG- -ORG- fili...</td>\n",
       "      <td>COMPLAINT against filing fee $ 400 , receipt n...</td>\n",
       "      <td>complaint fee receipt number disclosure civil ...</td>\n",
       "      <td>Complaints, Service of Process</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case assigned to Judge Ann M Donnelly and Magi...</td>\n",
       "      <td>Individual Practices of the assigned Judges</td>\n",
       "      <td>Ann M Donnelly Vera M. Scanlon Bowens Priscilla</td>\n",
       "      <td>Case assigned to Judge -NAME- -NAME- -NAME- an...</td>\n",
       "      <td>Case assigned to Judge and Magistrate Judge . ...</td>\n",
       "      <td>please download locate website responsible cou...</td>\n",
       "      <td></td>\n",
       "      <td>please download locate website responsible cou...</td>\n",
       "      <td>please_download_locate_website responsible_cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summons Issued as to Cardiogenics Holdings, In...</td>\n",
       "      <td>Cardiogenics Holdings</td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>Summons Issued as to -ORG- -ORG- , Inc.. ( -NA...</td>\n",
       "      <td>Summons Issued as to , Inc.. ( , ) ( Entered :...</td>\n",
       "      <td>summons inc</td>\n",
       "      <td>Service of Process</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td></td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "      <td>notice email miss civil cover sheet</td>\n",
       "      <td>Notices</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td></td>\n",
       "      <td>Bowens Priscilla</td>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "      <td>accordance rule federal rule civil procedure l...</td>\n",
       "      <td>Notices, Motions</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text  \\\n",
       "0  COMPLAINT against Cardiogenics Holdings, Inc. ...   \n",
       "1  Case assigned to Judge Ann M Donnelly and Magi...   \n",
       "2  Summons Issued as to Cardiogenics Holdings, In...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                              Organization Portion  \\\n",
       "0  Cardiogenics Holdings , Inc. LG Capital Funding   \n",
       "1      Individual Practices of the assigned Judges   \n",
       "2                            Cardiogenics Holdings   \n",
       "3                                                    \n",
       "4                                                    \n",
       "\n",
       "                                      Name Portion  \\\n",
       "0             ( Steinmetz Michael Bowens Priscilla   \n",
       "1  Ann M Donnelly Vera M. Scanlon Bowens Priscilla   \n",
       "2                                 Bowens Priscilla   \n",
       "3                                 Bowens Priscilla   \n",
       "4                                 Bowens Priscilla   \n",
       "\n",
       "                            Identifying Org and Name  \\\n",
       "0  COMPLAINT against -ORG- -ORG- -ORG- -ORG- fili...   \n",
       "1  Case assigned to Judge -NAME- -NAME- -NAME- an...   \n",
       "2  Summons Issued as to -ORG- -ORG- , Inc.. ( -NA...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                               Stripped Org and Name  \\\n",
       "0  COMPLAINT against filing fee $ 400 , receipt n...   \n",
       "1  Case assigned to Judge and Magistrate Judge . ...   \n",
       "2  Summons Issued as to , Inc.. ( , ) ( Entered :...   \n",
       "3  NOTICE - emailed attorney regarding missing se...   \n",
       "4  In accordance with Rule 73 of the Federal Rule...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \\\n",
       "0  complaint fee receipt number disclosure civil ...   \n",
       "1  please download locate website responsible cou...   \n",
       "2                                        summons inc   \n",
       "3                notice email miss civil cover sheet   \n",
       "4  accordance rule federal rule civil procedure l...   \n",
       "\n",
       "                        DT Topics  \\\n",
       "0  Complaints, Service of Process   \n",
       "1                                   \n",
       "2              Service of Process   \n",
       "3                         Notices   \n",
       "4                Notices, Motions   \n",
       "\n",
       "                  Removed unnecessary POS & vocab DT  \\\n",
       "0                                                      \n",
       "1  please download locate website responsible cou...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                          Apply Trigram Phrase Model  \n",
       "0                                                     \n",
       "1  please_download_locate_website responsible_cou...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original Docket Text', 'Organization Portion', 'Name Portion',\n",
       "       'Identifying Org and Name', 'Stripped Org and Name',\n",
       "       'Removed unnecessary POS & vocab', 'DT Topics',\n",
       "       'Removed unnecessary POS & vocab DT', 'Apply Trigram Phrase Model'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[['Original Docket Text', 'Removed unnecessary POS & vocab', \n",
    "        'Removed unnecessary POS & vocab DT', 'DT Topics',\n",
    "        'Apply Trigram Phrase Model']].to_csv('NEW_NLP_output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3203, 9)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3203"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Original Docket Text'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop here if you don't need to compare with previous NLP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "      <th>New Topocs</th>\n",
       "      <th>Action [Y/N]</th>\n",
       "      <th>If Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(REDACTED) RULE 56.1 STATEMENT. Document filed...</td>\n",
       "      <td>['rule .', 'document file .']</td>\n",
       "      <td>['rule .', 'document file .']</td>\n",
       "      <td>['rule .', 'document file .']</td>\n",
       "      <td>System Msg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>***DELETED DOCUMENT. Deleted document number 1...</td>\n",
       "      <td>['delete document .', 'delete document number ...</td>\n",
       "      <td>['delete document .', 'delete document number ...</td>\n",
       "      <td>['delete document .', 'delete document number ...</td>\n",
       "      <td>System Msg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***DELETED DOCUMENT. Deleted document number 2...</td>\n",
       "      <td>['delete document .', 'delete document number ...</td>\n",
       "      <td>['delete document .', 'delete document number ...</td>\n",
       "      <td>['delete document .', 'delete document number ...</td>\n",
       "      <td>System Msg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>***NOTE TO ATTORNEY OF NON-ECF CASE ERROR. Not...</td>\n",
       "      <td>['note nonecf error .', 'note manually refile ...</td>\n",
       "      <td>['note nonecf error .', 'note manually refile ...</td>\n",
       "      <td>['note nonecf error .', 'note manually refile ...</td>\n",
       "      <td>System Msg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...</td>\n",
       "      <td>['note refile document nonecf error .', 'note ...</td>\n",
       "      <td>['note refile document nonecf error .', 'note ...</td>\n",
       "      <td>['note refile document nonecf error .', 'note ...</td>\n",
       "      <td>System Msg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text  \\\n",
       "0  (REDACTED) RULE 56.1 STATEMENT. Document filed...   \n",
       "1  ***DELETED DOCUMENT. Deleted document number 1...   \n",
       "2  ***DELETED DOCUMENT. Deleted document number 2...   \n",
       "3  ***NOTE TO ATTORNEY OF NON-ECF CASE ERROR. Not...   \n",
       "4  ***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \\\n",
       "0                      ['rule .', 'document file .']   \n",
       "1  ['delete document .', 'delete document number ...   \n",
       "2  ['delete document .', 'delete document number ...   \n",
       "3  ['note nonecf error .', 'note manually refile ...   \n",
       "4  ['note refile document nonecf error .', 'note ...   \n",
       "\n",
       "                  Removed unnecessary POS & vocab DT  \\\n",
       "0                      ['rule .', 'document file .']   \n",
       "1  ['delete document .', 'delete document number ...   \n",
       "2  ['delete document .', 'delete document number ...   \n",
       "3  ['note nonecf error .', 'note manually refile ...   \n",
       "4  ['note refile document nonecf error .', 'note ...   \n",
       "\n",
       "                          Apply Trigram Phrase Model  New Topocs Action [Y/N]  \\\n",
       "0                      ['rule .', 'document file .']  System Msg            N   \n",
       "1  ['delete document .', 'delete document number ...  System Msg            N   \n",
       "2  ['delete document .', 'delete document number ...  System Msg            N   \n",
       "3  ['note nonecf error .', 'note manually refile ...  System Msg            N   \n",
       "4  ['note refile document nonecf error .', 'note ...  System Msg            N   \n",
       "\n",
       "  If Y  \n",
       "0  NaN  \n",
       "1  NaN  \n",
       "2  NaN  \n",
       "3  NaN  \n",
       "4  NaN  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noDT_data = pd.read_excel(r'E:\\WinUser\\Documents\\Python Code\\AI Paralegal\\docket_texts\\Train\\DT\\New Topics - Classification -5.27.2018.xlsx')\n",
    "noDT_data.drop('DT Topics', axis = 1, inplace = True)\n",
    "noDT_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old data shape: (624, 7)\n",
      "old data no duplicates shape: (602, 7)\n",
      "new data shape: (3244, 9)\n",
      "new data no duplicates shape: (3203, 9)\n",
      "new data no DT: 596\n"
     ]
    }
   ],
   "source": [
    "print('old data shape: {}'.format(noDT_data.shape))\n",
    "print('old data no duplicates shape: {}'.format(noDT_data.drop_duplicates().shape))\n",
    "print('new data shape: {}'.format(new_df.shape))\n",
    "print('new data no duplicates shape: {}'.format(new_df.drop_duplicates().shape))\n",
    "print('new data no DT: {}'.format((new_df.drop_duplicates()['DT Topics'] == '').sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603, 15)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = new_df[new_df['DT Topics'] == ''].drop_duplicates().merge(noDT_data.drop_duplicates(), how = 'outer', on = 'Original Docket Text')\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('old_v_new_study.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original Docket Text', 'Removed unnecessary POS & vocab',\n",
       "       'Removed unnecessary POS & vocab DT', 'Apply Trigram Phrase Model',\n",
       "       'New Topocs', 'Action [Y/N]', 'If Y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noDT_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original Docket Text', 'Organization Portion', 'Name Portion',\n",
       "       'Identifying Org and Name', 'Stripped Org and Name',\n",
       "       'Removed unnecessary POS & vocab_x', 'DT Topics',\n",
       "       'Removed unnecessary POS & vocab DT_x', 'Apply Trigram Phrase Model_x',\n",
       "       'Removed unnecessary POS & vocab_y',\n",
       "       'Removed unnecessary POS & vocab DT_y', 'Apply Trigram Phrase Model_y',\n",
       "       'New Topocs', 'Action [Y/N]', 'If Y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab_x</th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT_x</th>\n",
       "      <th>Apply Trigram Phrase Model_x</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab_y</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT_y</th>\n",
       "      <th>Apply Trigram Phrase Model_y</th>\n",
       "      <th>New Topocs</th>\n",
       "      <th>Action [Y/N]</th>\n",
       "      <th>If Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Original Docket Text, Organization Portion, Name Portion, Identifying Org and Name, Stripped Org and Name, Removed unnecessary POS & vocab_x, DT Topics, Removed unnecessary POS & vocab DT_x, Apply Trigram Phrase Model_x, Removed unnecessary POS & vocab_y, Removed unnecessary POS & vocab DT_y, Apply Trigram Phrase Model_y, New Topocs, Action [Y/N], If Y]\n",
       "Index: []"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[(combined_df['DT Topics'] != '') & (combined_df['Action [Y/N]'] == '')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".to_csv('old_v_new_study.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "      <th>New Topocs</th>\n",
       "      <th>Action [Y/N]</th>\n",
       "      <th>If Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>MEMO ENDORSEMENT on re: (37 in 1:04-cv-07900-L...</td>\n",
       "      <td>['memo endorsement motion entry order hereto a...</td>\n",
       "      <td>['memo endorsement motion entry order hereto a...</td>\n",
       "      <td>['memo_endorsement motion entry order hereto a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Original Docket Text  \\\n",
       "242  MEMO ENDORSEMENT on re: (37 in 1:04-cv-07900-L...   \n",
       "\n",
       "                       Removed unnecessary POS & vocab  \\\n",
       "242  ['memo endorsement motion entry order hereto a...   \n",
       "\n",
       "                    Removed unnecessary POS & vocab DT  \\\n",
       "242  ['memo endorsement motion entry order hereto a...   \n",
       "\n",
       "                            Apply Trigram Phrase Model New Topocs  \\\n",
       "242  ['memo_endorsement motion entry order hereto a...        NaN   \n",
       "\n",
       "    Action [Y/N] If Y  \n",
       "242          NaN  NaN  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"MEMO ENDORSEMENT on re: (37 in 1:04-cv-07900-LAK, (72 in 1:03-cv-02387-LAK) MOTION for an entry of an order in the form attached hereto as Exhibit 1 attached to this motion. ENDORSED: Granted. So Ordered (Signed by Judge Lewis A. Kaplan on 7/29/2008) Filed In Associated Cases: 1:03-cv-02387-LAK, 1:04-cv-07900-LAK(jfe) (Entered: 07/29/2008)\"\n",
    "noDT_data[noDT_data['Original Docket Text'] == text1]\n",
    "#Chris' input: 1) Y, 2) Court's Order, 3) Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>MEMO ENDORSEMENT on re: (37 in 1:04-cv-07900-L...</td>\n",
       "      <td></td>\n",
       "      <td>Lewis A. Kaplan</td>\n",
       "      <td>MEMO ENDORSEMENT on re : ( 37 in 1:04-cv-07900...</td>\n",
       "      <td>MEMO ENDORSEMENT on re : ( 37 in 1:04-cv-07900...</td>\n",
       "      <td>memo endorsement motion entry order hereto att...</td>\n",
       "      <td></td>\n",
       "      <td>memo endorsement motion entry order hereto att...</td>\n",
       "      <td>memo_endorsement motion entry order hereto att...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Original Docket Text Organization Portion  \\\n",
       "679  MEMO ENDORSEMENT on re: (37 in 1:04-cv-07900-L...                        \n",
       "\n",
       "        Name Portion                           Identifying Org and Name  \\\n",
       "679  Lewis A. Kaplan  MEMO ENDORSEMENT on re : ( 37 in 1:04-cv-07900...   \n",
       "\n",
       "                                 Stripped Org and Name  \\\n",
       "679  MEMO ENDORSEMENT on re : ( 37 in 1:04-cv-07900...   \n",
       "\n",
       "                       Removed unnecessary POS & vocab DT Topics  \\\n",
       "679  memo endorsement motion entry order hereto att...             \n",
       "\n",
       "                    Removed unnecessary POS & vocab DT  \\\n",
       "679  memo endorsement motion entry order hereto att...   \n",
       "\n",
       "                            Apply Trigram Phrase Model  \n",
       "679  memo_endorsement motion entry order hereto att...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df['Original Docket Text'] == text1]\n",
    "#Chris' input: 1) Y, 2) Court's Order, 3) Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "      <th>New Topocs</th>\n",
       "      <th>Action [Y/N]</th>\n",
       "      <th>If Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Original Docket Text, Removed unnecessary POS & vocab, Removed unnecessary POS & vocab DT, Apply Trigram Phrase Model, New Topocs, Action [Y/N], If Y]\n",
       "Index: []"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"DECISION and ORDER: Pursuant to 28 U.S.C.  1404, this Court hereby transfers this matter to the United States District Court for the Southern District of New York. Case transferred to District of Southern District of New York. Original file, certified copy of transfer order, and docket sheet sent. ALL FILINGS ARE TO BE MADE IN THE TRANSFER COURT, DO NOT DOCKET TO THIS CASE.. So Ordered by Judge William F. Kuntz, II on 4/17/2017. (Tavarez, Jennifer) [Transferred from New York Eastern on 4/28/2017.] (Entered: 04/19/2017)\"\n",
    "noDT_data[noDT_data['Original Docket Text'] == text1]\n",
    "#Chris' input: 1) Y, 2) Court's Order, 3) Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>DECISION and ORDER: Pursuant to 28 U.S.C.  14...</td>\n",
       "      <td>United States District Court for the Southern ...</td>\n",
       "      <td>William F. Kuntz Tavarez Jennifer</td>\n",
       "      <td>DECISION and ORDER : Pursuant to 28 U.S.C .  ...</td>\n",
       "      <td>DECISION and ORDER : Pursuant to 28 U.S.C .  ...</td>\n",
       "      <td>decision order u c  transfer transfer origina...</td>\n",
       "      <td></td>\n",
       "      <td>decision order u c  transfer transfer origina...</td>\n",
       "      <td>decision order u_c  transfer transfer origina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Original Docket Text  \\\n",
       "733  DECISION and ORDER: Pursuant to 28 U.S.C.  14...   \n",
       "\n",
       "                                  Organization Portion  \\\n",
       "733  United States District Court for the Southern ...   \n",
       "\n",
       "                          Name Portion  \\\n",
       "733  William F. Kuntz Tavarez Jennifer   \n",
       "\n",
       "                              Identifying Org and Name  \\\n",
       "733  DECISION and ORDER : Pursuant to 28 U.S.C .  ...   \n",
       "\n",
       "                                 Stripped Org and Name  \\\n",
       "733  DECISION and ORDER : Pursuant to 28 U.S.C .  ...   \n",
       "\n",
       "                       Removed unnecessary POS & vocab DT Topics  \\\n",
       "733  decision order u c  transfer transfer origina...             \n",
       "\n",
       "                    Removed unnecessary POS & vocab DT  \\\n",
       "733  decision order u c  transfer transfer origina...   \n",
       "\n",
       "                            Apply Trigram Phrase Model  \n",
       "733  decision order u_c  transfer transfer origina...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df['Original Docket Text'] == text2]\n",
    "#Chris' input: 1) Y, 2) Court's Order, 3) Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "      <th>New Topocs</th>\n",
       "      <th>Action [Y/N]</th>\n",
       "      <th>If Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...</td>\n",
       "      <td>['note refile document nonecf document error ....</td>\n",
       "      <td>['note refile document nonecf document error ....</td>\n",
       "      <td>['note refile document nonecf document error ....</td>\n",
       "      <td>System Msg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Original Docket Text  \\\n",
       "6  ***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...   \n",
       "\n",
       "                     Removed unnecessary POS & vocab  \\\n",
       "6  ['note refile document nonecf document error ....   \n",
       "\n",
       "                  Removed unnecessary POS & vocab DT  \\\n",
       "6  ['note refile document nonecf document error ....   \n",
       "\n",
       "                          Apply Trigram Phrase Model  New Topocs Action [Y/N]  \\\n",
       "6  ['note refile document nonecf document error ....  System Msg          NaN   \n",
       "\n",
       "  If Y  \n",
       "6  NaN  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = \"***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-ECF DOCUMENT ERROR. Note to Attorney Gary A. Bornstein: Document No. 329 is an Exhibit. This document is not filed via ECF. Exhibits are ONLY filed as attachments to a supporting or opposing document. (ldi) (Entered: 01/02/2013)\"\n",
    "noDT_data[noDT_data['Original Docket Text'] == text3]\n",
    "#Chris' input: 1) Y, 2) Court's Order, 3) Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Docket Text</th>\n",
       "      <th>Organization Portion</th>\n",
       "      <th>Name Portion</th>\n",
       "      <th>Identifying Org and Name</th>\n",
       "      <th>Stripped Org and Name</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab</th>\n",
       "      <th>DT Topics</th>\n",
       "      <th>Removed unnecessary POS &amp; vocab DT</th>\n",
       "      <th>Apply Trigram Phrase Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...</td>\n",
       "      <td>ECF</td>\n",
       "      <td>Gary A. Bornstein</td>\n",
       "      <td>***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...</td>\n",
       "      <td>***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...</td>\n",
       "      <td>note refile nonecf note via support oppose</td>\n",
       "      <td></td>\n",
       "      <td>note refile nonecf note via support oppose</td>\n",
       "      <td>note refile nonecf note via support oppose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Original Docket Text Organization Portion  \\\n",
       "1391  ***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...                  ECF   \n",
       "\n",
       "           Name Portion                           Identifying Org and Name  \\\n",
       "1391  Gary A. Bornstein  ***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...   \n",
       "\n",
       "                                  Stripped Org and Name  \\\n",
       "1391  ***NOTE TO ATTORNEY TO RE-FILE DOCUMENT - NON-...   \n",
       "\n",
       "                 Removed unnecessary POS & vocab DT Topics  \\\n",
       "1391  note refile nonecf note via support oppose             \n",
       "\n",
       "              Removed unnecessary POS & vocab DT  \\\n",
       "1391  note refile nonecf note via support oppose   \n",
       "\n",
       "                      Apply Trigram Phrase Model  \n",
       "1391  note refile nonecf note via support oppose  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df['Original Docket Text'] == text3]\n",
    "#Chris' input: 1) Y, 2) Court's Order, 3) Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
