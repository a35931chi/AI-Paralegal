{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_original = list(new_df['Original Docket Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(5):\n",
    "    org_str = []\n",
    "    name_str = []\n",
    "    stripped_str1 = []\n",
    "    stripped_str2 = []\n",
    "\n",
    "    tokens = word_tokenize(docket_original[i])\n",
    "    for SFlabel, NLlabel, token in zip(tagger.tag(tokens), tree2conlltags(ne_chunk(pos_tag(tokens))), tokens):\n",
    "        print(SFlabel, NLlabel, token)\n",
    "        if SFlabel[1] == 'ORGANIZATION':\n",
    "            org_str.append(SFlabel[0])\n",
    "            stripped_str1.append('-ORG-')\n",
    "        elif SFlabel[1] == 'PERSON':\n",
    "            name_str.append(SFlabel[0])\n",
    "            stripped_str1.append('-NAME-')\n",
    "        else:\n",
    "            stripped_str1.append(token)\n",
    "            stripped_str2.append(token)\n",
    "\n",
    "    output.append([docket_original[i],\n",
    "                   ' '.join(org_str),\n",
    "                   ' '.join(name_str),\n",
    "                   ' '.join(stripped_str1),\n",
    "                   ' '.join(stripped_str2)])\n",
    "    \n",
    "for i in range(5):\n",
    "    print('docket text:', i)\n",
    "    print(output[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'docket_texts/train/DT/basic_df.pickle'\n",
    "#to load\n",
    "with open(filename, 'rb') as handle:\n",
    "    NER_df = pickle.load(handle)\n",
    "    \n",
    "new_df = NER_df.copy()\n",
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize\n",
    "def normalize(docket_original):\n",
    "\n",
    "    url_regex1 = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    url_regex2 = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    date_regex = '(\\d{1,2}[\\/ ](\\d{2}|January|Jan|February|Feb|March|Mar|April|Apr|May|May|June|Jun|July|Jul|August|Aug|September|Sep|October|Oct|November|Nov|December|Dec)[\\/ ]\\d{2,4})'\n",
    "    punct_regex = r\"[^a-zA-Z0-9]\"\n",
    "    num_regex = \"\\d+\"\n",
    "    extraspace_regex = \" +\"\n",
    "    docket_original = docket_original.split(' ')\n",
    "    docket_normalized = [text.lower() for text in docket_original]\n",
    "    docket_nourl1 = [re.sub(url_regex1, \"url\", text) for text in docket_normalized]\n",
    "    docket_nourl2 = [re.sub(url_regex2, \"url\", text) for text in docket_nourl1]\n",
    "    docket_nodate = [re.sub(date_regex, \"date\", text) for text in docket_nourl2]\n",
    "    docket_nopunct = [re.sub(punct_regex, \" \", text) for text in docket_nodate]\n",
    "    docket_nonum = [re.sub(num_regex, \" \", text) for text in docket_nopunct]\n",
    "    docket_noextraspace = re.sub(extraspace_regex, \" \", ' '.join(docket_nonum))\n",
    "    \n",
    "    return docket_noextraspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "new_df['normalized'] = new_df['Stripped Org and Name'].apply(lambda x: normalize(x))\n",
    "print('took {} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(docket_noextraspace):\n",
    "    return word_tokenize(docket_noextraspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "new_df['tokenize'] = new_df['normalized'].apply(tokenize)\n",
    "print('took {} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(docket_tokenized):  \n",
    "    return [word for word in docket_tokenized if word not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "new_df['nostop'] = new_df['tokenize'].apply(stopwords)\n",
    "print('took {} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is updated, comparing mannual_topics_20180828 and manual_topics_20180911 and manual_topics_20180919\n",
    "manual_topics_df = pd.read_csv('manual_topics_20180919.csv') \n",
    "manual_topics_df = manual_topics_df.apply(lambda x: x.astype(str).str.lower())\n",
    "manual_topics_dict = manual_topics_df.to_dict('list')\n",
    "for topic in manual_topics_dict.keys():\n",
    "    manual_topics_dict[topic] = [keyword for keyword in manual_topics_dict[topic] if keyword != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is updated, comparing mannual_topics_20180828 and manual_topics_20180911 and manual_topics_20180919\n",
    "manual_topics_df = pd.read_csv('manual_topics_20180919.csv') \n",
    "manual_topics_df = manual_topics_df.apply(lambda x: x.astype(str).str.lower())\n",
    "manual_topics_dict = manual_topics_df.to_dict('list')\n",
    "for topic in manual_topics_dict.keys():\n",
    "    manual_topics_dict[topic] = [keyword for keyword in manual_topics_dict[topic] if keyword != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output a list of topics\n",
    "def mannual_topic_assignment(text):\n",
    "    #text = text.split()\n",
    "    #print(text)\n",
    "    output = []\n",
    "    for topic in manual_topics_dict.keys():\n",
    "        for keyword in manual_topics_dict[topic]:\n",
    "            if ' '.join(text).find(keyword) != -1:\n",
    "                output.append(topic)\n",
    "    #print(output)\n",
    "    #if there's motion to dismiss, then remove noaction\n",
    "    if ' '.join(text).find('motion dismiss') != -1 and 'NoAction' in output:\n",
    "        output = list(set(output))\n",
    "        output.remove('NoAction')\n",
    "    return ', '.join(set(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_texts_output = list(new_df['nostop'])\n",
    "\n",
    "docket_texts_output_DT = []\n",
    "topics_DT = []\n",
    "NoAction = []\n",
    "\n",
    "for text in docket_texts_output:\n",
    "    topic = mannual_topic_assignment(text)\n",
    "    #print(text, topic)\n",
    "    if topic != '':\n",
    "        docket_texts_output_DT.append('')\n",
    "        topics_DT.append(topic)\n",
    "        if 'NoAction' in topic:\n",
    "            NoAction.append(1)\n",
    "        else:\n",
    "            NoAction.append(0)\n",
    "    else:\n",
    "        docket_texts_output_DT.append(text)\n",
    "        topics_DT.append('')\n",
    "        NoAction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['DT Topics'] = pd.Series(topics_DT)\n",
    "new_df['NoAction'] = pd.Series(NoAction)\n",
    "new_df['Removed unnecessary POS & vocab DT'] = pd.Series(docket_texts_output_DT)\n",
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(docket_nostop):\n",
    "    return ' '.join([WordNetLemmatizer().lemmatize(word, pos='v') for word in docket_nostop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "new_df['lemmed'] = new_df['Removed unnecessary POS & vocab DT'].apply(lemm)\n",
    "print('took {} seconds'.format(time.time() - t0))\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_phrase2 = list(new_df['lemmed'])\n",
    "docket_phrase2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = 'docket_texts/train/DT/unigram_nltk_newsop2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# turn the lemmatized corpus into unigram sentences\n",
    "with codecs.open(unigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for sentence in docket_phrase2:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = 'docket_texts/train/DT/bigram_model_newsop2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# store our bigram model\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk if we don't want to run this again\n",
    "#bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = 'docket_texts/train/DT/bigram_sentences_newsop2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# apply the bigram model, and write it to file\n",
    "with codecs.open(bigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nUnigram sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, 0, 10):\n",
    "    print(' '.join(unigram_sentence))\n",
    "print('\\nBigram sentence:')\n",
    "for bigram_sentence in it.islice(bigram_sentences, 0, 10):\n",
    "    print(' '.join(bigram_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = 'docket_texts/train/DT/trigram_model_newsop2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = 'docket_texts/train/DT/trigram_sentences_newsop2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open(trigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "        #print('Bi', bigram_sentence)\n",
    "        trigram_sentence = ' '.join(trigram_model[bigram_sentence])\n",
    "        #print('Tri', trigram_sentence)\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "finish = 5\n",
    "print('Original text:')\n",
    "print(docket_phrase2[start:finish], '\\n')\n",
    "\n",
    "print('\\nUNIGRAM Sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, start, finish):\n",
    "    print(' '.join(unigram_sentence))\n",
    "print('\\nBIGRAM Sentence:')\n",
    "for bigram_sentence in it.islice(bigram_sentences, start, finish):\n",
    "    print(' '.join(bigram_sentence))\n",
    "print('\\nTRIGRAM Sentence:')\n",
    "for trigram_sentence in it.islice(trigram_sentences, start, finish):\n",
    "    print(' '.join(trigram_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_transform(texts):\n",
    "    display = False\n",
    "    texts = str(texts)\n",
    "    trigram_output = ''\n",
    "    #print(texts)\n",
    "\n",
    "    remove_trigram = ['calendar_day', 'court_notice_intend', 'minute_entry_proceeding_hold', 'court_reportertranscriber_abovecaptioned_matter',\n",
    "                      'redaction_calendar_day', 'rule_statement', 'obtain_pacer', 'may_obtain_pacer', 'reportertranscriber_abovecaptioned_matter',\n",
    "                      'redact_transcript_deadline', 'send_chamber', \"official_transcript_notice_give\", \"notice_intent_request\", \"proceed_hold\", \n",
    "                      \"fee_receipt_number\", \"civil_procedure\", \"pursuant_frcp\", \"official_transcript_conference\", \n",
    "                      \"purchase_reportertranscriber_deadline_release\", \"et_al\", \"mail_chamber\", \"transcript_restriction\", \"redaction_transcript\", \n",
    "                      \"transcript_view_public_terminal\", \"transcript_make_remotely\", \"associated_et_al\", \"electronically_available_public_without\", \n",
    "                      \"genesys_id\", \"release_transcript_restriction\", \"adar_bay\", \"redaction_request_due\", \"new_york\", \"official_transcript_conference\", \n",
    "                      \"transcript_make_remotely\", \"transcript_proceeding_conference_hold\", \"redaction_transcript\",\n",
    "                      'affidavit_jr._c.p.a', 'corporate_parent', 'certain_underwriter', 'federal_rule_civil_procedure', 'redaction_request', \n",
    "                      'official_transcript', 'rule_disclosure', 'rule_corporate_disclosure', 'place_vault', 'public_without_redaction_calendar', \n",
    "                      'purchase_deadline_release_transcript', 'transcript_proceeding_hold', 'transcript_remotely_electronically_available',\n",
    "                      'minute_entry_hold', 'discovery_hear_hold', 'jury_trial_hold', \"sign_judge\",'place_vault']\n",
    "\n",
    "    if texts == None:\n",
    "        return None\n",
    "    \n",
    "    unigram_review = []\n",
    "    for word in texts.split():\n",
    "        unigram_review.append(word)\n",
    "    if display:\n",
    "        print('Uni: ', unigram_review)\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    if display:\n",
    "        print('Bi: ', bigram_review)\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    if display:\n",
    "        print('Tri: ', trigram_review)\n",
    "    trigram_review = [phrase for phrase in trigram_review if phrase not in remove_trigram]\n",
    "    if display:\n",
    "        print('Tri removed: ', trigram_review)\n",
    "    trigram_output += ' '.join(trigram_review)\n",
    "    \n",
    "    return trigram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_phrase3 = [trigram_transform(text) for text in docket_phrase2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docket_phrase2[:5])\n",
    "print(docket_phrase3[:5])\n",
    "len(set(docket_phrase3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['phrases'] = pd.Series(docket_phrase3)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "print('orignal text: ')\n",
    "print(new_df['Original Docket Text'].iloc[i], '\\n')\n",
    "print('org and name removed: ')\n",
    "print(new_df['Stripped Org and Name'].iloc[i], '\\n')\n",
    "print('normalized: ')\n",
    "print(new_df['normalized'].iloc[i], '\\n')\n",
    "print('stopwords removed: ')\n",
    "print(' '.join(new_df['nostop'].iloc[i]), '\\n')\n",
    "print('after lemmetization: ')\n",
    "print(new_df['lemmed'].iloc[i], '\\n')\n",
    "print('after phrase modeling: ')\n",
    "print(new_df['phrases'].iloc[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pos = list(pd.read_excel('NLP_to_be_removed.xlsx', sheetname = 0, header = None)[0])\n",
    "remove_word = list(pd.read_excel('NLP_to_be_removed.xlsx', sheetname = 1, header = None)[0])\n",
    "remove_trigram = list(pd.read_excel('NLP_to_be_removed.xlsx', sheetname = 2, header = None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(text):\n",
    "    new_text = []\n",
    "    text = text.split(' ')\n",
    "    for word in text:\n",
    "        if word not in remove_word and word not in remove_trigram:\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['cleaned'] = new_df['phrases'].apply(remove_unnecessary)\n",
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'docket_texts/train/DT/basic_cleaned.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save\n",
    "with open(filename, 'wb') as handle: \n",
    "    pickle.dump(new_df, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[['Original Docket Text', 'normalized', 'DT Topics', 'lemmed', 'phrases']].drop_duplicates().to_csv('Check if keywords working.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape before dedupe: {}'.format(new_df.shape))\n",
    "print('shape after dedupe: {}'.format(new_df[['Original Docket Text', 'normalized', 'DT Topics', 'lemmed', 'phrases']].drop_duplicates().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chris_file = pd.read_excel(r'temporary file - Sept 19.xlsx')\n",
    "print(chris_file.shape)\n",
    "chris_file.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_df = new_df[['Original Docket Text', 'normalized', 'lemmed', 'phrases', 'DT Topics', 'NoAction']].drop_duplicates().merge(chris_file[['Original Docket Text', 'Action', 'If Action']], on = 'Original Docket Text', how = 'left')\n",
    "print(another_df.shape)\n",
    "another_df.tail(1)\n",
    "#another_df.to_excel('tempfile.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_df['Action revised'] = np.where(another_df['NoAction'] == 1, 'N', another_df['Action'])\n",
    "another_df['If Action revised'] = np.where(another_df['NoAction'] == 1, '', another_df['If Action'])\n",
    "\n",
    "another_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to check\n",
    "another_df.to_excel('tempfile.xlsx', index = False)\n",
    "#applying methodology to the testing file is at another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
