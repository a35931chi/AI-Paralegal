{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#visualization libraries\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = 'C:/Program Files/Java/jdk-10.0.1/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import itertools as it\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import corpus/docket texts from html to pandas DataFrame\n",
    "def grab_docket_test():\n",
    "    files = []\n",
    "    #get all .html files in the folder (all docket files are in .html)\n",
    "    for file in os.listdir('docket_texts/test/'):\n",
    "        if file.endswith('.html'):\n",
    "            files.append(os.path.join('docket_texts/test/', file))\n",
    "\n",
    "    df_docket_texts = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(files)): #gather all docket texts\n",
    "    #for i in [0, 1]: #for testing purposes\n",
    "        \n",
    "        content = codecs.open(files[i], 'r', 'utf-8').read()\n",
    "        #use beautiful soup to get the case ID\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        case_id = str(soup.find_all('h3'))    \n",
    "        bookmark1 = case_id.find('CASE #:') + len('CASE #:')\n",
    "        bookmark2 = case_id.find('</h3>')\n",
    "        case_id = case_id[bookmark1:bookmark2]\n",
    "\n",
    "        #use pandas to grab tables in the html files\n",
    "        docket_tables = pd.read_html(content)\n",
    "\n",
    "        #error checking: gotta do this because there's different length of docket_list/\n",
    "        #usually docket texts are in docket_list[3], but not always\n",
    "        n = 0\n",
    "        while docket_tables[n].isin(['Docket Text']).sum().sum() == 0:\n",
    "            #print(n, docket_tables[n].isin(['Docket Text']).sum().sum())\n",
    "            n += 1\n",
    "                        \n",
    "        #print(i, files[i])\n",
    "        #print(docket_tables[n].head())\n",
    "\n",
    "        #docket_tables[n] is the docket text table\n",
    "        new_header = docket_tables[n].iloc[0]\n",
    "        docket_tables[n] = docket_tables[n][1:]\n",
    "        docket_tables[n].columns = new_header\n",
    "        \n",
    "        docket_tables[n]['#'] = pd.to_numeric(docket_tables[n]['#'],\n",
    "                                              downcast = 'signed', errors = 'coerce')\n",
    "        docket_tables[n]['Date Filed'] = pd.to_datetime(docket_tables[n]['Date Filed'])\n",
    "        docket_tables[n]['Case ID'] = case_id\n",
    "\n",
    "        df_docket_texts = pd.concat([df_docket_texts, docket_tables[n]])\n",
    "    #reorder a column\n",
    "    cols = list(df_docket_texts.columns)\n",
    "    df_docket_texts = df_docket_texts[[cols[-1]] + cols[:-1]]\n",
    "    \n",
    "    print('current docket text table size/shape: {}'.format(df_docket_texts.shape))\n",
    "    return df_docket_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n",
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current docket text table size/shape: (1804, 4)\n",
      "docket text 0\n",
      "FILING ERROR - DEFICIENT PLEADING - FILED AGAINST PARTY ERROR COMPLAINT against All Defendants. (Filing Fee $ 400.00, Receipt Number 0208-11793625)Document filed by Majid Soueidan.(Rosen, Samuel) Modified on 1/5/2016 (pc). (Entered: 01/04/2016) \n",
      "\n",
      "docket text 1\n",
      "FILING ERROR PDF ERROR CIVIL COVER SHEET filed. (Rosen, Samuel) Modified on 1/5/2016 (pc). (Entered: 01/04/2016) \n",
      "\n",
      "docket text 2\n",
      "REQUEST FOR ISSUANCE OF SUMMONS as to BREEZE-EASTERN CORPORATION, BRAD PEDERSEN, ROBERT J. KELLY, NELSON OBUS, WILLIAM M. SHOCKLEY, and SERGE DUPUIS, re: 1 Complaint. Document filed by Majid Soueidan. (Rosen, Samuel) (Entered: 01/04/2016) \n",
      "\n",
      "docket text 3\n",
      "RULE 7.1 CORPORATE DISCLOSURE STATEMENT. No Corporate Parent. Document filed by Majid Soueidan.(Rosen, Samuel) (Entered: 01/04/2016) \n",
      "\n",
      "docket text 4\n",
      "***NOTICE TO ATTORNEY REGARDING DEFICIENT PLEADING. Notice to Attorney Samuel Kenneth Rosen to RE-FILE Document No. 1 Complaint,. The filing is deficient for the following reason(s): the All Defendant radio button was selected. Re-file the pleading using the event type Complaint found under the event list Complaints and Other Initiating Documents - attach the correct signed PDF - select the individually named filer/filers - select the individually named party/parties the pleading is against. (pc) (Entered: 01/05/2016) \n",
      "\n",
      "Wall time: 2.15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\builder\\_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  self.parser.feed(markup)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = grab_docket_test()\n",
    "docket_original = list(df['Docket Text'])\n",
    "for i in range(5):\n",
    "    print('docket text {}'.format(i))\n",
    "    print(docket_original[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_to_model = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\classifiers\\english.all.3class.distsim.crf.ser.gz'\n",
    "path_to_jar = r'C:\\Users\\inves\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\nltk\\stanford-ner-2018-02-27\\stanford-ner.jar'\n",
    "tagger = StanfordNERTagger(path_to_model, path_to_jar = path_to_jar)\n",
    "\n",
    "output = []\n",
    "#length = 100 \n",
    "length = len(docket_original)\n",
    "for i in range(length):\n",
    "    org_str = []\n",
    "    name_str = []\n",
    "    stripped_str1 = []\n",
    "    stripped_str2 = []\n",
    "    tokens = nltk.tokenize.word_tokenize(docket_original[i])\n",
    "    for label in tagger.tag(tokens):\n",
    "        #print(label)\n",
    "        if label[1] == 'ORGANIZATION':\n",
    "            org_str.append(label[0])\n",
    "            stripped_str1.append('-ORG-')\n",
    "        elif label[1] == 'PERSON':\n",
    "            name_str.append(label[0])\n",
    "            stripped_str1.append('-NAME-')\n",
    "        else:\n",
    "            stripped_str1.append(label[0])\n",
    "            stripped_str2.append(label[0])\n",
    "    \n",
    "    output.append([docket_original[i],\n",
    "                   ' '.join(org_str),\n",
    "                   ' '.join(name_str),\n",
    "                   ' '.join(stripped_str1),\n",
    "                   ' '.join(stripped_str2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df = pd.DataFrame(output, columns = ['Original Docket Text', 'Organization Portion', 'Name Portion', \n",
    "                                         'Identifying Org and Name', 'Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = NER_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df.head())\n",
    "docket_text_list = list(new_df['Stripped Org and Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess1(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('(s)', 's')\n",
    "    text = text.replace(\"'s\", 's')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.replace('&', ' ')\n",
    "    return text\n",
    "\n",
    "def text_preprocess2(text):\n",
    "    text = text.replace('.', '')\n",
    "    return text\n",
    "\n",
    "def remove_stop(sentence):\n",
    "    output = []\n",
    "    for word in sentence.split():\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            output.append(word)\n",
    "    return ' '.join(output)\n",
    "\n",
    "keywords = pd.read_csv('docket_texts/keywords.csv', header = None)\n",
    "keywords.columns = ['keywords']\n",
    "keyword_list = list(keywords['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docket_text_list[1])\n",
    "docket_text_list = [text_preprocess1(sentence).lower() for sentence in docket_text_list]\n",
    "print(docket_text_list[1])\n",
    "print(len(docket_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(remove_stop(sent)) for sent in sentences]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class LemmatizationWithPOSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_wordnet_pos(self,treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def pos_tag(self,tokens):\n",
    "        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "\n",
    "        # lemmatization using pos tagg   \n",
    "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
    "        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
    "        return pos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = Splitter()\n",
    "lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n",
    "\n",
    "lemma_docket_text_list = []\n",
    "for docket_text in docket_text_list:\n",
    "    #step 1 split document into sentence followed by tokenization\n",
    "    tokens = splitter.split(docket_text)\n",
    "\n",
    "    #step 2 lemmatization using pos tagger \n",
    "    lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "    lemma_docket_text_list.append(lemma_pos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lemma_docket_text_list)) #docket text document level\n",
    "print(len(lemma_docket_text_list[0])) #docket text sentence level\n",
    "print(len(lemma_docket_text_list[0][0])) #docket text word level\n",
    "print(lemma_docket_text_list[0][0][0]) #docket text token level\n",
    "print(lemma_docket_text_list[0][0][0][0]) #docket text tuple level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets do a collection of what we have\n",
    "collection = {}\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    for sentence in lemma_pos_token:\n",
    "        for token in sentence:\n",
    "            #print(token[2][0])\n",
    "            if token[2][0] not in list(collection.keys()):\n",
    "                collection[token[2][0]] = []\n",
    "                collection[token[2][0]].append(token[1])\n",
    "            else:\n",
    "                if token[1] not in collection[token[2][0]]:\n",
    "                    collection[token[2][0]].append(token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict([ (k, pd.Series(v)) for k, v in collection.items()])).to_csv('NLP_pos.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "remove_pos = [\"``\", \"NNPS\", \"NNP\", \"CD\", '#', '$', \"''\", \",\", \"0\", \":\"]\n",
    "remove_word = [\"'s\", \"judge\", \"party\", \"defendant\", \"ex\", \"plantiff\", \"shall\", \"date\", \"b\", \"exhibit\", \"pennsylvania\", \"sign_judge\", \n",
    "               \"Inc..\", \"inc..\", \"llc\", \"'\", \"[_]\", \"action\", \"clerk\", \"july\", \"kw\", \"regard\", \"sac\", \"attachment\", \"c.d\", \"cal\", \"case\", \"cd\", \"l.p.\", \n",
    "               \"claim\", \"copy\", \"court\", \"direct\", \"form\", \"hereby\", \"magistrate\", \"p.c\", \"pl\", \"plaintiff\", \"regard\", \"sign\", \"time\", \"mr.\", \n",
    "               \"docket\", \"follow\", \"set\", \"matter\" \"agreement\" \"proceeding\", \"cotton\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \n",
    "               \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "               \"agreement\", \"v.\", \"place_vault\", \"modify\", \"fund\", \"associated\", \"provide\", \"material\", \"amount\", \"accordingly\", \"additional\", \n",
    "               \"second\", \"esq\", \"transmission\", \"g.c.\", \"seal\", \"review\", \"honor\", \"submit\", \"counsel\", \"witness\", \"civ\", \"first\", \"ltd..\", \"enter\", \n",
    "               \"stay\", \"forth\", \"matter\", \"whether\", \"class\", \"master\", \"information\", \"statement\", \"submission\", \"related\", \"see\", \"make\", \"paper\", \n",
    "               \"brookfield\", \"designate\", \"remain\", \"reportertranscriber\", \"submit\", \"include\", \"mail\", \"fact\", \"refer\", \"take\", \"pursuant\", \"amount\", \n",
    "               \"behalf\", \"I.p..\", \"must\", \"attorney\",\n",
    "               'abovecapitoned', 'attach', 'add', 'concern', 'chamber', 'close', 'district', 'damage', 'later', \n",
    "               'relate', 'return', 'require', 'restriction', 'respect', 'ny', 'seek', 'write', 'expert', 'transcript', \n",
    "               'day', 'h.o', 'damage', 'pre', 'proceeding', 'present', 'page', 'pending', 'p.m.', 'frcp', 'g.c.', 'record', 'r.']\n",
    "\n",
    "    \n",
    "#rebuild corpus\n",
    "docket_texts_output = [] #ultimate output after cleaning\n",
    "\n",
    "for lemma_pos_token in lemma_docket_text_list:\n",
    "    docket_text_output = [] \n",
    "    for sentence in lemma_pos_token:\n",
    "        sentence_output = []\n",
    "        for token in sentence:\n",
    "            #print(token[1])\n",
    "            \n",
    "            if token[2][0] not in remove_pos: #if the pos is not in the remove_pos list\n",
    "                if token[1] not in remove_word: #these are the intentionally left out words\n",
    "                    sentence_output.append(token[1]) #append the the sentence\n",
    "        docket_text_output.append(' '.join(sentence_output))\n",
    "    docket_texts_output.append(docket_text_output)\n",
    "print(docket_texts_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Removed unnecessary POS & vocab'] = pd.Series(docket_texts_output)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_transform(texts):\n",
    "    trigram_output = []\n",
    "    #print(texts)\n",
    "    remove_trigram = ['calendar_day', 'court_notice_intend', 'minute_entry_proceeding_hold', 'court_reportertranscriber_abovecaptioned_matter',\n",
    "                      'redaction_calendar_day', 'rule_statement', 'obtain_pacer', 'may_obtain_pacer', 'reportertranscriber_abovecaptioned_matter',\n",
    "                      'redact_transcript_deadline', 'send_chamber', \"official_transcript_notice_give\", \"notice_intent_request\", \"proceed_hold\", \n",
    "                      \"fee_receipt_number\", \"civil_procedure\", \"pursuant_frcp\", \"official_transcript_conference\", \n",
    "                      \"purchase_reportertranscriber_deadline_release\", \"et_al\", \"mail_chamber\", \"transcript_restriction\", \"redaction_transcript\", \n",
    "                      \"transcript_view_public_terminal\", \"transcript_make_remotely\", \"associated_et_al\", \"electronically_available_public_without\", \n",
    "                      \"genesys_id\", \"release_transcript_restriction\", \"adar_bay\", \"redaction_request_due\", \"new_york\", \"official_transcript_conference\", \n",
    "                      \"transcript_make_remotely\", \"transcript_proceeding_conference_hold\", \"redaction_transcript\",\n",
    "                      'affidavit_jr._c.p.a', 'corporate_parent', 'certain_underwriter', 'federal_rule_civil_procedure', 'redaction_request', \n",
    "                      'official_transcript', 'rule_disclosure', 'rule_corporate_disclosure', 'place_vault', 'public_without_redaction_calendar', \n",
    "                      'purchase_deadline_release_transcript', 'transcript_proceeding_hold', 'transcript_remotely_electronically_available']\n",
    "  \n",
    "    \n",
    "    for sentence in texts:\n",
    "        unigram_review = []\n",
    "        for word in sentence.split():\n",
    "            unigram_review.append(word)\n",
    "    \n",
    "        #print('Uni: ', unigram_review)\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        #print('Bi: ', bigram_review)\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "        trigram_review = [phrase for phrase in trigram_review if phrase not in remove_trigram]\n",
    "        #print('Tri: ', trigram_review)\n",
    "        trigram_output.append(' '.join(trigram_review))\n",
    "    return trigram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Apply Trigram Phrase Model'] = new_df['Removed unnecessary POS & vocab'].apply(trigram_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Apply Trigram Phrase Model'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write trigram to file\n",
    "trigram_dockets_filepath = 'docket_texts/test/trigram_transformed_dockets_noorgnoname.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(trigram_dockets_filepath, 'w', encoding= 'utf_8') as f:\n",
    "    for i in range(len(new_df['Apply Trigram Phrase Model'])):\n",
    "        f.write(' '.join(new_df['Apply Trigram Phrase Model'][i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = 'docket_texts/train/trigram_dict_noorgnoname.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#some dictionary hyperparameters:\n",
    "no_below = 10 #reference is 10\n",
    "no_above = 0.4 #reference is 0.4\n",
    "\n",
    "trigram_reviews = LineSentence(trigram_dockets_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare otrigram_reviewsr too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below = no_below, no_above = no_above) #this step is questionable. May need to change the parameters\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "#trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = 'docket_texts/trigram_bow_corpus_noorgnoname.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        #print(review)\n",
    "        #print(trigram_dictionary.doc2bow(review))\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# generate bag-of-words representations for\n",
    "# all reviews and save them as a matrix\n",
    "MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_sentences_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "print(trigram_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
