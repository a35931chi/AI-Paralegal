{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA background\n",
    "LDA assumes that documents are probability distribution over laten topics.\n",
    "Topics are probability distribution over words.\n",
    "LDA takes a number of documents. It assumes that the words in each document are related. It then tries to figure out the 'recipe' for how each document could have been created. We just need to tell the model how many topics to construct and it uses that 'recipe' to generate topic and word distributions over a corpus. Based on that output, we can identify similar documents within the corpus.\n",
    "\n",
    "### In order to understand the LDA process, we have to know how LDA assumes topics are generated:\n",
    "1. determine the number of words in the document\n",
    "2. choose a topic mixture for the document over a fixed set of topics (ie. topic A 20%, topic B 50%, etc)\n",
    "3. generate words in the document by:\n",
    "    - pick a topic based on the document's multinomial distribution\n",
    "    - pick a word based on the topic's multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working backwards\n",
    "Suppose you have a corpus of documents, and you want LDA to learn the topic representatino of K topics in each document and the word distribution of each topic. LDA would backtrack from the document level to identify topics that are likely to have generated the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA's Magic\n",
    "1. randomly assign each word in each documen tto one of the K topics\n",
    "2. for each document\n",
    "    - assume that all topic assignments except for the current one are correct\n",
    "    - claculate two proportions:\n",
    "        1. proportion of words in document d that are currently assigned to topic t = p(topic t | document d)\n",
    "        2. proportion of assignments to topic t over all documents that come from this word w = p(word w | topic t)\n",
    "    - multiply those two proportions and assign w a new topic based on that probability. p(topic t | document d) * p(word w | topic t)\n",
    "3. eventually we'll reach a steady state where assignments make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alpha (parameter of the Dirichlet pior of the per-document topic distribution)\n",
    "high: each document will contain many topics\n",
    "low: each document iwll have distinct topics\n",
    "\n",
    "### beta (parameter of the Dirichlet prior on the per-topic word distribution)\n",
    "high: each topic will contain many words\n",
    "low: each topic will contain few words\n",
    "\n",
    "### theta (topic distribution for document m)\n",
    "### z (topic for the n-th word in document m)\n",
    "### w (specific word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 0: examine and import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import itertools as it\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from gensim.models import Phrases #seems like this is slower, but Phaser was not compatible to our code? need some research\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_dockets():\n",
    "    files = []\n",
    "    #get all .html files in the folder (all docket files are in .html)\n",
    "    for file in os.listdir('docket_texts/'):\n",
    "        if file.endswith('.html'):\n",
    "            files.append(os.path.join('docket_texts/', file))\n",
    "\n",
    "    df_docket_texts = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(files)): #gather all docket texts\n",
    "    #for i in [0, 1]: #for testing purposes\n",
    "        \n",
    "        content = codecs.open(files[i], 'r', 'utf-8').read()\n",
    "        #use beautiful soup to get the case ID\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        case_id = str(soup.find_all('h3'))    \n",
    "        bookmark1 = case_id.find('CASE #:') + len('CASE #:')\n",
    "        bookmark2 = case_id.find('</h3>')\n",
    "        case_id = case_id[bookmark1:bookmark2]\n",
    "\n",
    "        #use pandas to grab tables in the html files\n",
    "        docket_tables = pd.read_html(content)\n",
    "\n",
    "        #error checking: gotta do this because there's different length of docket_list/\n",
    "        #usually docket texts are in docket_list[3], but not always\n",
    "        n = 0\n",
    "        while docket_tables[n].isin(['Docket Text']).sum().sum() == 0:\n",
    "            #print(n, docket_tables[n].isin(['Docket Text']).sum().sum())\n",
    "            n += 1\n",
    "                        \n",
    "        #print(i, files[i])\n",
    "        #print(docket_tables[n].head())\n",
    "\n",
    "        #docket_tables[n] is the docket text table\n",
    "        new_header = docket_tables[n].iloc[0]\n",
    "        docket_tables[n] = docket_tables[n][1:]\n",
    "        docket_tables[n].columns = new_header\n",
    "        \n",
    "        docket_tables[n]['#'] = pd.to_numeric(docket_tables[n]['#'],\n",
    "                                              downcast = 'signed', errors = 'coerce')\n",
    "        docket_tables[n]['Date Filed'] = pd.to_datetime(docket_tables[n]['Date Filed'])\n",
    "        docket_tables[n]['Case ID'] = case_id\n",
    "\n",
    "        df_docket_texts = pd.concat([df_docket_texts, docket_tables[n]])\n",
    "    #reorder a column\n",
    "    cols = list(df_docket_texts.columns)\n",
    "    df_docket_texts = df_docket_texts[[cols[-1]] + cols[:-1]]\n",
    "    \n",
    "    print('current docket text table size/shape: {}'.format(df_docket_texts.shape))\n",
    "    return df_docket_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current docket text table size/shape: (721, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Date Filed</th>\n",
       "      <th>#</th>\n",
       "      <th>Docket Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1:16-cv-01215-AMD-SJB</td>\n",
       "      <td>2016-03-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLAINT against Cardiogenics Holdings, Inc. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1:16-cv-01215-AMD-SJB</td>\n",
       "      <td>2016-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Case assigned to Judge Ann M Donnelly and Magi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1:16-cv-01215-AMD-SJB</td>\n",
       "      <td>2016-03-10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Summons Issued as to Cardiogenics Holdings, In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1:16-cv-01215-AMD-SJB</td>\n",
       "      <td>2016-03-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOTICE - emailed attorney regarding missing se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1:16-cv-01215-AMD-SJB</td>\n",
       "      <td>2016-03-11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>In accordance with Rule 73 of the Federal Rule...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                 Case ID Date Filed    #  \\\n",
       "1   1:16-cv-01215-AMD-SJB 2016-03-10  1.0   \n",
       "2   1:16-cv-01215-AMD-SJB 2016-03-10  NaN   \n",
       "3   1:16-cv-01215-AMD-SJB 2016-03-10  2.0   \n",
       "4   1:16-cv-01215-AMD-SJB 2016-03-11  NaN   \n",
       "5   1:16-cv-01215-AMD-SJB 2016-03-11  3.0   \n",
       "\n",
       "0                                        Docket Text  \n",
       "1  COMPLAINT against Cardiogenics Holdings, Inc. ...  \n",
       "2  Case assigned to Judge Ann M Donnelly and Magi...  \n",
       "3  Summons Issued as to Cardiogenics Holdings, In...  \n",
       "4  NOTICE - emailed attorney regarding missing se...  \n",
       "5  In accordance with Rule 73 of the Federal Rule...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#current docket text table size/shape: (721, 4), 2018-04-18\n",
    "df = grab_dockets()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMPLAINT against Cardiogenics Holdings, Inc. filing fee $ 400, receipt number 0207-8445206 Was the Disclosure Statement on Civil Cover Sheet completed -YES,, filed by LG Capital Funding, LLC. (Steinmetz, Michael) (Additional attachment(s) added on 3/11/2016: # 1 Civil Cover Sheet, # 2 Proposed Summons) (Bowens, Priscilla). (Entered: 03/10/2016)', 'Case assigned to Judge Ann M Donnelly and Magistrate Judge Vera M. Scanlon. Please download and review the Individual Practices of the assigned Judges, located on our website. Attorneys are responsible for providing courtesy copies to judges where their Individual Practices require such. (Bowens, Priscilla) (Entered: 03/11/2016)']\n"
     ]
    }
   ],
   "source": [
    "docket_original = list(df['Docket Text'])\n",
    "print(docket_original[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_txt_filepath = 'docket_texts/docket_text_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 721 docket texts written to the new txt file\n",
      "Wall time: 5.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# if we need to update file, then True\n",
    "if True:\n",
    "    \n",
    "    docket_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(docket_txt_filepath, 'w', encoding = 'utf_8') as docket_txt_file:\n",
    "\n",
    "\n",
    "        for i in range(len(docket_original)):\n",
    "            docket_txt_file.write(docket_original[i] + '\\n')\n",
    "            docket_count += 1\n",
    "\n",
    "    print('Text from {:,} docket texts written to the new txt file'.format(docket_count))\n",
    "    \n",
    "else:\n",
    "    with codecs.open(docket_txt_filepath, encoding = 'utf_8') as docket_txt_file:\n",
    "        for review_count, line in enumerate(docket_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print(u'Text from {:,} restaurant reviews in the txt file.'.format(docket_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXHIBIT C - Notice of Conversion by LG Capital Funding, LLC. Related document: 1 Complaint, filed by LG Capital Funding, LLC. (Kehrli, Kevin) (Entered: 04/11/2016)\n",
      "(721, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df['Docket Text'].iloc[9])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTION for Extension of Time to File Answer by Cardiogenics Holdings, Inc. (Kogan, Simon) Modified on 4/13/2016 to add motion (Quinlan, Krista). (Entered: 04/12/2016)\n",
      "\n",
      "Sentence 1:\n",
      "MOTION for Extension of Time to File Answer by Cardiogenics Holdings, Inc.\n",
      "\n",
      "Sentence 2:\n",
      "(Kogan, Simon)\n",
      "\n",
      "Sentence 3:\n",
      "Modified on 4/13/2016 to add motion (Quinlan, Krista).\n",
      "\n",
      "Sentence 4:\n",
      "(Entered: 04/12/2016)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting a review for testing\n",
    "n_th_review = 10\n",
    "with codecs.open(docket_txt_filepath, encoding = 'utf_8') as f:\n",
    "    sample_review = list(it.islice(f, n_th_review, n_th_review + 1))[0]\n",
    "\n",
    "print(sample_review)\n",
    "\n",
    "#using NLP (SpaCy English) to parse the review text\n",
    "parsed_review = nlp(sample_review)\n",
    "\n",
    "#for example, we can print out the sentences from the parsed object\n",
    "#why were the sentences split by a dash?!\n",
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: AFFIDAVIT OF SERVICE of ( - ORG\n",
      "\n",
      "Entity 2: 2 - CARDINAL\n",
      "\n",
      "Entity 3: 3 - CARDINAL\n",
      "\n",
      "Entity 4: Chris Han - PERSON\n",
      "\n",
      "Entity 5: 4 - CARDINAL\n",
      "\n",
      "Entity 6: NCP - ORG\n",
      "\n",
      "Entity 7: New York - GPE\n",
      "\n",
      "Entity 8: 4/11/2018 - DATE\n",
      "\n",
      "Entity 9: Thomas Paglia - PERSON\n",
      "\n",
      "Entity 10: Hangzhou - GPE\n",
      "\n",
      "Entity 11: Kailai Neckwear Apparel Co. Ltd. - ORG\n",
      "\n",
      "Entity 12: Han - NORP\n",
      "\n",
      "Entity 13: Chris - PERSON\n",
      "\n",
      "Entity 14: Entered - PERSON\n",
      "\n",
      "Entity 15: \n",
      " - GPE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we can also look at entities, note that this result is not great\n",
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFFIDAVIT</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OF</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Notice</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motion</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Default</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Judgment</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Against</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>All</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defendants</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>;</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Proposed</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Judgment</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>;</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Declaration</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chris</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Han</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>was</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>accepted</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Paglia</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Officer</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Document</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>filed</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Hangzhou</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Kailai</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Neckwear</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Apparel</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Co.</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ltd.</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Han</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Chris</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Entered</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>04/13/2018</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text part_of_speech\n",
       "0      AFFIDAVIT          PROPN\n",
       "1             OF            ADP\n",
       "2        SERVICE          PROPN\n",
       "3             of            ADP\n",
       "4              (          PUNCT\n",
       "5              1          PUNCT\n",
       "6              )          PUNCT\n",
       "7         Notice          PROPN\n",
       "8             of            ADP\n",
       "9         Motion          PROPN\n",
       "10           for            ADP\n",
       "11       Default          PROPN\n",
       "12      Judgment          PROPN\n",
       "13       Against            ADP\n",
       "14           All            DET\n",
       "15    Defendants          PROPN\n",
       "16             ;          PUNCT\n",
       "17             (          PUNCT\n",
       "18             2          PUNCT\n",
       "19             )          PUNCT\n",
       "20      Proposed           VERB\n",
       "21      Judgment           NOUN\n",
       "22             ;          PUNCT\n",
       "23             (          PUNCT\n",
       "24             3          PUNCT\n",
       "25             )          PUNCT\n",
       "26   Declaration          PROPN\n",
       "27            of            ADP\n",
       "28         Chris          PROPN\n",
       "29           Han          PROPN\n",
       "..           ...            ...\n",
       "80           was           VERB\n",
       "81      accepted           VERB\n",
       "82            by            ADP\n",
       "83        Thomas          PROPN\n",
       "84        Paglia          PROPN\n",
       "85             ,          PUNCT\n",
       "86       Officer          PROPN\n",
       "87             .          PUNCT\n",
       "88      Document           NOUN\n",
       "89         filed           VERB\n",
       "90            by            ADP\n",
       "91      Hangzhou          PROPN\n",
       "92        Kailai          PROPN\n",
       "93      Neckwear          PROPN\n",
       "94       Apparel          PROPN\n",
       "95           Co.          PROPN\n",
       "96          Ltd.          PROPN\n",
       "97             ,          PUNCT\n",
       "98             .          PUNCT\n",
       "99             (          PUNCT\n",
       "100          Han          PROPN\n",
       "101            ,          PUNCT\n",
       "102        Chris          PROPN\n",
       "103            )          PUNCT\n",
       "104            (          PUNCT\n",
       "105      Entered           VERB\n",
       "106            :          PUNCT\n",
       "107   04/13/2018          PUNCT\n",
       "108            )          PUNCT\n",
       "109           \\n          SPACE\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review] #Verbatim text content \n",
    "token_pos = [token.pos_ for token in parsed_review] #part-of-speech.\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_pos)), columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFFIDAVIT</td>\n",
       "      <td>affidavit</td>\n",
       "      <td>XXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OF</td>\n",
       "      <td>of</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE</td>\n",
       "      <td>service</td>\n",
       "      <td>XXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Notice</td>\n",
       "      <td>notice</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motion</td>\n",
       "      <td>motion</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Default</td>\n",
       "      <td>default</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Judgment</td>\n",
       "      <td>judgment</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Against</td>\n",
       "      <td>against</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>All</td>\n",
       "      <td>all</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defendants</td>\n",
       "      <td>defendants</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>;</td>\n",
       "      <td>;</td>\n",
       "      <td>;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Proposed</td>\n",
       "      <td>propose</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Judgment</td>\n",
       "      <td>judgment</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>;</td>\n",
       "      <td>;</td>\n",
       "      <td>;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Declaration</td>\n",
       "      <td>declaration</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chris</td>\n",
       "      <td>chris</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Han</td>\n",
       "      <td>han</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>accepted</td>\n",
       "      <td>accept</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>thomas</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Paglia</td>\n",
       "      <td>paglia</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Officer</td>\n",
       "      <td>officer</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Document</td>\n",
       "      <td>document</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>filed</td>\n",
       "      <td>file</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Hangzhou</td>\n",
       "      <td>hangzhou</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Kailai</td>\n",
       "      <td>kailai</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Neckwear</td>\n",
       "      <td>neckwear</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Apparel</td>\n",
       "      <td>apparel</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Co.</td>\n",
       "      <td>co.</td>\n",
       "      <td>Xx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ltd.</td>\n",
       "      <td>ltd.</td>\n",
       "      <td>Xxx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Han</td>\n",
       "      <td>han</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Chris</td>\n",
       "      <td>chris</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Entered</td>\n",
       "      <td>enter</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>04/13/2018</td>\n",
       "      <td>04/13/2018</td>\n",
       "      <td>dd/dd/dddd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text  token_lemma token_shape\n",
       "0      AFFIDAVIT    affidavit        XXXX\n",
       "1             OF           of          XX\n",
       "2        SERVICE      service        XXXX\n",
       "3             of           of          xx\n",
       "4              (            (           (\n",
       "5              1            1           d\n",
       "6              )            )           )\n",
       "7         Notice       notice       Xxxxx\n",
       "8             of           of          xx\n",
       "9         Motion       motion       Xxxxx\n",
       "10           for          for         xxx\n",
       "11       Default      default       Xxxxx\n",
       "12      Judgment     judgment       Xxxxx\n",
       "13       Against      against       Xxxxx\n",
       "14           All          all         Xxx\n",
       "15    Defendants   defendants       Xxxxx\n",
       "16             ;            ;           ;\n",
       "17             (            (           (\n",
       "18             2            2           d\n",
       "19             )            )           )\n",
       "20      Proposed      propose       Xxxxx\n",
       "21      Judgment     judgment       Xxxxx\n",
       "22             ;            ;           ;\n",
       "23             (            (           (\n",
       "24             3            3           d\n",
       "25             )            )           )\n",
       "26   Declaration  declaration       Xxxxx\n",
       "27            of           of          xx\n",
       "28         Chris        chris       Xxxxx\n",
       "29           Han          han         Xxx\n",
       "..           ...          ...         ...\n",
       "80           was           be         xxx\n",
       "81      accepted       accept        xxxx\n",
       "82            by           by          xx\n",
       "83        Thomas       thomas       Xxxxx\n",
       "84        Paglia       paglia       Xxxxx\n",
       "85             ,            ,           ,\n",
       "86       Officer      officer       Xxxxx\n",
       "87             .            .           .\n",
       "88      Document     document       Xxxxx\n",
       "89         filed         file        xxxx\n",
       "90            by           by          xx\n",
       "91      Hangzhou     hangzhou       Xxxxx\n",
       "92        Kailai       kailai       Xxxxx\n",
       "93      Neckwear     neckwear       Xxxxx\n",
       "94       Apparel      apparel       Xxxxx\n",
       "95           Co.          co.         Xx.\n",
       "96          Ltd.         ltd.        Xxx.\n",
       "97             ,            ,           ,\n",
       "98             .            .           .\n",
       "99             (            (           (\n",
       "100          Han          han         Xxx\n",
       "101            ,            ,           ,\n",
       "102        Chris        chris       Xxxxx\n",
       "103            )            )           )\n",
       "104            (            (           (\n",
       "105      Entered        enter       Xxxxx\n",
       "106            :            :           :\n",
       "107   04/13/2018   04/13/2018  dd/dd/dddd\n",
       "108            )            )           )\n",
       "109           \\n           \\n          \\n\n",
       "\n",
       "[110 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review] # Base form of the token, with no inflectional suffixes.\n",
    "token_shape = [token.shape_ for token in parsed_review] # Transform of the tokens's string, to show orthographic features. For example, \"Xxxx\" or \"dd\".\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)), columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFFIDAVIT</td>\n",
       "      <td>ORG</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OF</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Notice</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motion</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Default</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Judgment</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Against</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>All</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defendants</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>;</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Proposed</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Judgment</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>;</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Declaration</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>of</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chris</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Han</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>accepted</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>by</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Paglia</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Officer</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Document</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>filed</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>by</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Hangzhou</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Kailai</td>\n",
       "      <td>ORG</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Neckwear</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Apparel</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Co.</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ltd.</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Han</td>\n",
       "      <td>NORP</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Chris</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Entered</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>:</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>04/13/2018</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>\\n</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text entity_type inside_outside_begin\n",
       "0      AFFIDAVIT         ORG                    B\n",
       "1             OF         ORG                    I\n",
       "2        SERVICE         ORG                    I\n",
       "3             of         ORG                    I\n",
       "4              (         ORG                    I\n",
       "5              1                                O\n",
       "6              )                                O\n",
       "7         Notice                                O\n",
       "8             of                                O\n",
       "9         Motion                                O\n",
       "10           for                                O\n",
       "11       Default                                O\n",
       "12      Judgment                                O\n",
       "13       Against                                O\n",
       "14           All                                O\n",
       "15    Defendants                                O\n",
       "16             ;                                O\n",
       "17             (                                O\n",
       "18             2    CARDINAL                    B\n",
       "19             )                                O\n",
       "20      Proposed                                O\n",
       "21      Judgment                                O\n",
       "22             ;                                O\n",
       "23             (                                O\n",
       "24             3    CARDINAL                    B\n",
       "25             )                                O\n",
       "26   Declaration                                O\n",
       "27            of                                O\n",
       "28         Chris      PERSON                    B\n",
       "29           Han      PERSON                    I\n",
       "..           ...         ...                  ...\n",
       "80           was                                O\n",
       "81      accepted                                O\n",
       "82            by                                O\n",
       "83        Thomas      PERSON                    B\n",
       "84        Paglia      PERSON                    I\n",
       "85             ,                                O\n",
       "86       Officer                                O\n",
       "87             .                                O\n",
       "88      Document                                O\n",
       "89         filed                                O\n",
       "90            by                                O\n",
       "91      Hangzhou         GPE                    B\n",
       "92        Kailai         ORG                    B\n",
       "93      Neckwear         ORG                    I\n",
       "94       Apparel         ORG                    I\n",
       "95           Co.         ORG                    I\n",
       "96          Ltd.         ORG                    I\n",
       "97             ,                                O\n",
       "98             .                                O\n",
       "99             (                                O\n",
       "100          Han        NORP                    B\n",
       "101            ,                                O\n",
       "102        Chris      PERSON                    B\n",
       "103            )                                O\n",
       "104            (                                O\n",
       "105      Entered      PERSON                    B\n",
       "106            :                                O\n",
       "107   04/13/2018                                O\n",
       "108            )                                O\n",
       "109           \\n         GPE                    B\n",
       "\n",
       "[110 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review] #Named entity type.\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review] #IOB code of named entity tag. \"B\" means the token begins an entity, \"I\" means it is inside an entity, \"O\" means it is outside an entity, and \"\" means no entity tag is set.\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFFIDAVIT</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OF</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Notice</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motion</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>for</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Default</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Judgment</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Against</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>All</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defendants</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>;</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Proposed</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Judgment</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>;</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Declaration</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>of</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chris</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Han</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>was</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>accepted</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>by</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Paglia</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>,</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Officer</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Document</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>filed</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>by</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Hangzhou</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Kailai</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Neckwear</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Apparel</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Co.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ltd.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>,</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Han</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>,</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Chris</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Entered</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>:</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>04/13/2018</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>\\n</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0      AFFIDAVIT            -20.0                                          \n",
       "1             OF            -20.0                                          \n",
       "2        SERVICE            -20.0                                          \n",
       "3             of            -20.0   Yes                                    \n",
       "4              (            -20.0                Yes                       \n",
       "5              1            -20.0                                    Yes   \n",
       "6              )            -20.0                Yes                       \n",
       "7         Notice            -20.0                                          \n",
       "8             of            -20.0   Yes                                    \n",
       "9         Motion            -20.0                                          \n",
       "10           for            -20.0   Yes                                    \n",
       "11       Default            -20.0                                          \n",
       "12      Judgment            -20.0                                          \n",
       "13       Against            -20.0                                          \n",
       "14           All            -20.0                                          \n",
       "15    Defendants            -20.0                                          \n",
       "16             ;            -20.0                Yes                       \n",
       "17             (            -20.0                Yes                       \n",
       "18             2            -20.0                                    Yes   \n",
       "19             )            -20.0                Yes                       \n",
       "20      Proposed            -20.0                                          \n",
       "21      Judgment            -20.0                                          \n",
       "22             ;            -20.0                Yes                       \n",
       "23             (            -20.0                Yes                       \n",
       "24             3            -20.0                                    Yes   \n",
       "25             )            -20.0                Yes                       \n",
       "26   Declaration            -20.0                                          \n",
       "27            of            -20.0   Yes                                    \n",
       "28         Chris            -20.0                                          \n",
       "29           Han            -20.0                                          \n",
       "..           ...              ...   ...          ...         ...     ...   \n",
       "80           was            -20.0   Yes                                    \n",
       "81      accepted            -20.0                                          \n",
       "82            by            -20.0   Yes                                    \n",
       "83        Thomas            -20.0                                          \n",
       "84        Paglia            -20.0                                          \n",
       "85             ,            -20.0                Yes                       \n",
       "86       Officer            -20.0                                          \n",
       "87             .            -20.0                Yes                       \n",
       "88      Document            -20.0                                          \n",
       "89         filed            -20.0                                          \n",
       "90            by            -20.0   Yes                                    \n",
       "91      Hangzhou            -20.0                                          \n",
       "92        Kailai            -20.0                                          \n",
       "93      Neckwear            -20.0                                          \n",
       "94       Apparel            -20.0                                          \n",
       "95           Co.            -20.0                                          \n",
       "96          Ltd.            -20.0                                          \n",
       "97             ,            -20.0                Yes                       \n",
       "98             .            -20.0                Yes                       \n",
       "99             (            -20.0                Yes                       \n",
       "100          Han            -20.0                                          \n",
       "101            ,            -20.0                Yes                       \n",
       "102        Chris            -20.0                                          \n",
       "103            )            -20.0                Yes                       \n",
       "104            (            -20.0                Yes                       \n",
       "105      Entered            -20.0                                          \n",
       "106            :            -20.0                Yes                       \n",
       "107   04/13/2018            -20.0                                          \n",
       "108            )            -20.0                Yes                       \n",
       "109           \\n            -20.0                            Yes           \n",
       "\n",
       "    out of vocab.?  \n",
       "0              Yes  \n",
       "1              Yes  \n",
       "2              Yes  \n",
       "3              Yes  \n",
       "4              Yes  \n",
       "5              Yes  \n",
       "6              Yes  \n",
       "7              Yes  \n",
       "8              Yes  \n",
       "9              Yes  \n",
       "10             Yes  \n",
       "11             Yes  \n",
       "12             Yes  \n",
       "13             Yes  \n",
       "14             Yes  \n",
       "15             Yes  \n",
       "16             Yes  \n",
       "17             Yes  \n",
       "18             Yes  \n",
       "19             Yes  \n",
       "20             Yes  \n",
       "21             Yes  \n",
       "22             Yes  \n",
       "23             Yes  \n",
       "24             Yes  \n",
       "25             Yes  \n",
       "26             Yes  \n",
       "27             Yes  \n",
       "28             Yes  \n",
       "29             Yes  \n",
       "..             ...  \n",
       "80             Yes  \n",
       "81             Yes  \n",
       "82             Yes  \n",
       "83             Yes  \n",
       "84             Yes  \n",
       "85             Yes  \n",
       "86             Yes  \n",
       "87             Yes  \n",
       "88             Yes  \n",
       "89             Yes  \n",
       "90             Yes  \n",
       "91             Yes  \n",
       "92             Yes  \n",
       "93             Yes  \n",
       "94             Yes  \n",
       "95             Yes  \n",
       "96             Yes  \n",
       "97             Yes  \n",
       "98             Yes  \n",
       "99             Yes  \n",
       "100            Yes  \n",
       "101            Yes  \n",
       "102            Yes  \n",
       "103            Yes  \n",
       "104            Yes  \n",
       "105            Yes  \n",
       "106            Yes  \n",
       "107            Yes  \n",
       "108            Yes  \n",
       "109            Yes  \n",
       "\n",
       "[110 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_, #Verbatim text content\n",
    "                     token.prob, #Smoothed log probability estimate of token's type\n",
    "                     token.is_stop, #Is the token part of a \"stop list\"\n",
    "                     token.is_punct, #Is the token punctuation\n",
    "                     token.is_space, #Does the token consist of whitespace characters\n",
    "                     token.like_num, #Does the token represent a number\n",
    "                     token.is_oov) #Is the token out-of-vocabulary\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df_temp = pd.DataFrame(token_attributes,\n",
    "                       columns=['text', 'log_probability', 'stop?', 'punctuation?',\n",
    "                                'whitespace?', 'number?', 'out of vocab.?'])\n",
    "\n",
    "df_temp.loc[:, 'stop?':'out of vocab.?'] = (df_temp.loc[:, 'stop?':'out of vocab.?']\n",
    "                                            .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we pretty much used SpaCy as text clean so far. We are going to use gensim to do phrase modeling\n",
    "\n",
    "1. Segment text of complete reviews into sentences & normalize text\n",
    "2. First-order phrase modeling → apply first-order phrase model to transform sentences\n",
    "3. Second-order phrase modeling → apply second-order phrase model to transform sentences\n",
    "4. Apply text normalization and second-order phrase model to text of complete reviews\n",
    "\n",
    "We'll use this transformed data as the input for some higher-level modeling approaches in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename), batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            #the problem is here where token.lemma_ transforms the pronouns\n",
    "            yield u' '.join([token.lemma_ for token in sent if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = 'docket_texts/unigram_sentences_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if True:\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding = 'utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(docket_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\n",
      "COMPLAINT against Cardiogenics Holdings, Inc. filing fee $ 400, receipt number 0207-8445206 Was the Disclosure Statement on Civil Cover Sheet completed -YES,, filed by LG Capital Funding, LLC. (Steinmetz, Michael) (Additional attachment(s) added on 3/11/2016: # 1 Civil Cover Sheet, # 2 Proposed Summons) (Bowens, Priscilla). (Entered: 03/10/2016)\n",
      "Case assigned to Judge Ann M Donnelly and Magistrate Judge Vera M. Scanlon. Please download and review the Individual Practices of the assigned Judges, located on our website. Attorneys are responsible for providing courtesy copies to judges where their Individual Practices require such. (Bowens, Priscilla) (Entered: 03/11/2016)\n",
      "\n",
      "unigram_sentence:\n",
      "complaint against cardiogenics holdings inc. filing fee $ 400 receipt number 0207\n",
      "\n",
      "8445206 be the disclosure statement on civil cover sheet complete -yes file by lg capital funding llc\n",
      "\n",
      "steinmetz michael\n",
      "\n",
      "additional attachment(s add on 3/11/2016 1 civil cover sheet 2 propose summon\n",
      "\n",
      "bowens priscilla\n",
      "\n",
      "enter 03/10/2016\n",
      "\n",
      "case assign to judge ann m donnelly and magistrate judge vera m. scanlon\n",
      "\n",
      "please download and review the individual practices of the assign judges locate on -PRON- website\n",
      "\n",
      "attorney be responsible for provide courtesy copy to judge where -PRON- individual practices require such\n",
      "\n",
      "bowens priscilla\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('original text:')\n",
    "print(df['Docket Text'].iloc[0])\n",
    "print(df['Docket Text'].iloc[1])\n",
    "\n",
    "print('\\nunigram_sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, 0, 10):\n",
    "    print(' '.join(unigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = 'docket_texts/bigram_model_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 92.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if True:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = 'docket_texts/bigram_sentences_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 210 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if True:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram length = 3456, bigram length = 3456\n"
     ]
    }
   ],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "print('unigram length = {}, bigram length = {}'.format(len(list(unigram_sentences)), len(list(bigram_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\n",
      "COMPLAINT against Cardiogenics Holdings, Inc. filing fee $ 400, receipt number 0207-8445206 Was the Disclosure Statement on Civil Cover Sheet completed -YES,, filed by LG Capital Funding, LLC. (Steinmetz, Michael) (Additional attachment(s) added on 3/11/2016: # 1 Civil Cover Sheet, # 2 Proposed Summons) (Bowens, Priscilla). (Entered: 03/10/2016)\n",
      "Case assigned to Judge Ann M Donnelly and Magistrate Judge Vera M. Scanlon. Please download and review the Individual Practices of the assigned Judges, located on our website. Attorneys are responsible for providing courtesy copies to judges where their Individual Practices require such. (Bowens, Priscilla) (Entered: 03/11/2016)\n",
      "\n",
      "unigram sentence:\n",
      "complaint against cardiogenics holdings inc. filing fee $ 400 receipt number 0207\n",
      "8445206 be the disclosure statement on civil cover sheet complete -yes file by lg capital funding llc\n",
      "steinmetz michael\n",
      "additional attachment(s add on 3/11/2016 1 civil cover sheet 2 propose summon\n",
      "bowens priscilla\n",
      "enter 03/10/2016\n",
      "case assign to judge ann m donnelly and magistrate judge vera m. scanlon\n",
      "please download and review the individual practices of the assign judges locate on -PRON- website\n",
      "attorney be responsible for provide courtesy copy to judge where -PRON- individual practices require such\n",
      "bowens priscilla\n",
      "\n",
      "bigram sentence:\n",
      "complaint_against cardiogenics_holdings inc. filing_fee $_400 receipt_number 0207\n",
      "8445206 be the disclosure_statement on civil_cover sheet_complete -yes file by_lg capital_funding llc\n",
      "steinmetz michael\n",
      "additional attachment(s add on 3/11/2016 1 civil_cover sheet 2 propose summon\n",
      "bowens_priscilla\n",
      "enter 03/10/2016\n",
      "case assign to judge_ann m_donnelly and magistrate_judge vera_m. scanlon\n",
      "please_download and_review the individual_practices of the assign_judges locate on -PRON-_website\n",
      "attorney be_responsible for provide_courtesy copy to judge_where -PRON-_individual practices_require such\n",
      "bowens_priscilla\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "finish = 10\n",
    "print('original text:')\n",
    "print(df['Docket Text'].iloc[0])\n",
    "print(df['Docket Text'].iloc[1])\n",
    "\n",
    "print('\\nunigram sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, 0, 10):\n",
    "    print(' '.join(unigram_sentence))\n",
    "print('\\nbigram sentence:')\n",
    "for bigram_sentence in it.islice(bigram_sentences, start, finish):\n",
    "    print(' '.join(bigram_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = 'docket_texts/trigram_model_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 87.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if True:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = 'docket_texts/trigram_sentences_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if True:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\n",
      "COMPLAINT against Cardiogenics Holdings, Inc. filing fee $ 400, receipt number 0207-8445206 Was the Disclosure Statement on Civil Cover Sheet completed -YES,, filed by LG Capital Funding, LLC. (Steinmetz, Michael) (Additional attachment(s) added on 3/11/2016: # 1 Civil Cover Sheet, # 2 Proposed Summons) (Bowens, Priscilla). (Entered: 03/10/2016)\n",
      "Case assigned to Judge Ann M Donnelly and Magistrate Judge Vera M. Scanlon. Please download and review the Individual Practices of the assigned Judges, located on our website. Attorneys are responsible for providing courtesy copies to judges where their Individual Practices require such. (Bowens, Priscilla) (Entered: 03/11/2016)\n",
      "\n",
      "unigram sentence:\n",
      "complaint against cardiogenics holdings inc. filing fee $ 400 receipt number 0207\n",
      "8445206 be the disclosure statement on civil cover sheet complete -yes file by lg capital funding llc\n",
      "steinmetz michael\n",
      "additional attachment(s add on 3/11/2016 1 civil cover sheet 2 propose summon\n",
      "bowens priscilla\n",
      "enter 03/10/2016\n",
      "case assign to judge ann m donnelly and magistrate judge vera m. scanlon\n",
      "please download and review the individual practices of the assign judges locate on -PRON- website\n",
      "attorney be responsible for provide courtesy copy to judge where -PRON- individual practices require such\n",
      "bowens priscilla\n",
      "\n",
      "bigram sentence:\n",
      "complaint_against cardiogenics_holdings inc. filing_fee $_400 receipt_number 0207\n",
      "8445206 be the disclosure_statement on civil_cover sheet_complete -yes file by_lg capital_funding llc\n",
      "steinmetz michael\n",
      "additional attachment(s add on 3/11/2016 1 civil_cover sheet 2 propose summon\n",
      "bowens_priscilla\n",
      "enter 03/10/2016\n",
      "case assign to judge_ann m_donnelly and magistrate_judge vera_m. scanlon\n",
      "please_download and_review the individual_practices of the assign_judges locate on -PRON-_website\n",
      "attorney be_responsible for provide_courtesy copy to judge_where -PRON-_individual practices_require such\n",
      "bowens_priscilla\n",
      "\n",
      "trigram sentence:\n",
      "complaint_against cardiogenics_holdings_inc. filing_fee_$_400 receipt_number_0207\n",
      "8445206 be the disclosure_statement on civil_cover_sheet_complete -yes file_by_lg capital_funding_llc\n",
      "steinmetz michael\n",
      "additional attachment(s add on 3/11/2016 1 civil_cover_sheet 2 propose summon\n",
      "bowens_priscilla\n",
      "enter 03/10/2016\n",
      "case assign to judge_ann_m_donnelly and magistrate_judge_vera_m. scanlon\n",
      "please_download_and_review the individual_practices of the assign_judges_locate on_-PRON-_website\n",
      "attorney_be_responsible for_provide_courtesy copy to judge_where_-PRON-_individual practices_require_such\n",
      "bowens_priscilla\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "finish = 10\n",
    "print('original text:')\n",
    "print(df['Docket Text'].iloc[0])\n",
    "print(df['Docket Text'].iloc[1])\n",
    "\n",
    "print('\\nunigram sentence:')\n",
    "for unigram_sentence in it.islice(unigram_sentences, 0, 10):\n",
    "    print(' '.join(unigram_sentence))\n",
    "print('\\nbigram sentence:')\n",
    "for bigram_sentence in it.islice(bigram_sentences, start, finish):\n",
    "    print(' '.join(bigram_sentence))\n",
    "print('\\ntrigram sentence:')\n",
    "for trigram_sentence in it.islice(trigram_sentences, start, finish):\n",
    "    print(' '.join(trigram_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write trigram to file\n",
    "trigram_dockets_filepath = 'docket_texts/trigram_transformed_dockets_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if True:\n",
    "\n",
    "    with codecs.open(trigram_dockets_filepath, 'w', encoding= 'utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(docket_txt_filepath), batch_size = 10000, n_threads = 4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review if term not in STOP_WORDS]\n",
    "            \n",
    "            #additionally we need to remove those pronouns\n",
    "            #trigram_review = [term for term in trigram_review if term != '-PRON-']\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = ' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "MOTION for Extension of Time to File Answer by Cardiogenics Holdings, Inc. (Kogan, Simon) Modified on 4/13/2016 to add motion (Quinlan, Krista). (Entered: 04/12/2016)\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "corresponding transformation: \n",
      "motion_for_extension time file answer cardiogenics_holdings_inc. kogan_simon modified_on 4/13/2016 add motion quinlan_krista enter 04/12/2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting a review for testing\n",
    "n_th_review = 10\n",
    "with codecs.open(docket_txt_filepath, encoding = 'utf_8') as f:\n",
    "    sample_review = list(it.islice(f, n_th_review, n_th_review + 1))[0]\n",
    "    \n",
    "print('Original:' + '\\n')\n",
    "print(sample_review)\n",
    "\n",
    "    \n",
    "# the transformed file shouldn't have any pronouns in it\n",
    "print('----' + '\\n')\n",
    "print('Transformed:' + '\\n') \n",
    "\n",
    "with codecs.open(trigram_dockets_filepath, encoding = 'utf_8') as f:\n",
    "    for review in it.islice(f, n_th_review, n_th_review + 1):\n",
    "        print('corresponding transformation: ')\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we are finally going to topic model with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = 'docket_texts/trigram_dict_all.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#some dictionary hyperparameters:\n",
    "no_below = 5\n",
    "no_above = 0.8\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if True:\n",
    "\n",
    "    trigram_reviews = LineSentence(trigram_dockets_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below = no_below, no_above = no_above) #this step is questionable. May need to change the parameters\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning the trigram dictionary into bag-of-words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = 'docket_texts/trigram_bow_corpus_all.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 80.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if True:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_sentences_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_filepath = 'docket_texts/lda_model_all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model Training\n",
    "note that we need couple of things:\n",
    "1. dictionary\n",
    "2. bow corpus\n",
    "3. numbre of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if True:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus, num_topics = 50, id2word = trigram_dictionary, workers = 3)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is to check out which token are assigned to which topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn = 10):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print('{:20} {}'.format('term', 'frequency') + '\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn = topn):\n",
    "        print('{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " topic 0's tokens:\n",
      "term                 frequency\n",
      "\n",
      "to                   0.038\n",
      "enter                0.036\n",
      "of                   0.035\n",
      "on                   0.032\n",
      "sign_by              0.024\n",
      "capital_funding_llc  0.024\n",
      "the                  0.022\n",
      "by                   0.020\n",
      "document_file_by_lg  0.020\n",
      "kehrli_kevin         0.018\n",
      "\n",
      " topic 1's tokens:\n",
      "term                 frequency\n",
      "\n",
      "the                  0.043\n",
      "and                  0.029\n",
      "on                   0.023\n",
      "for                  0.021\n",
      "a                    0.020\n",
      "be                   0.017\n",
      "of                   0.016\n",
      "ex                   0.015\n",
      "to                   0.015\n",
      "file                 0.015\n",
      "\n",
      " topic 2's tokens:\n",
      "term                 frequency\n",
      "\n",
      "to                   0.057\n",
      "by                   0.032\n",
      "enter                0.027\n",
      "motion               0.025\n",
      "the                  0.025\n",
      "and                  0.016\n",
      "llc                  0.016\n",
      "by_lg_capital_funding 0.016\n",
      "-PRON-               0.016\n",
      "file                 0.015\n",
      "\n",
      " topic 3's tokens:\n",
      "term                 frequency\n",
      "\n",
      "to                   0.119\n",
      "of                   0.047\n",
      "motion               0.044\n",
      "dismiss              0.020\n",
      "a                    0.019\n",
      "for                  0.018\n",
      "be                   0.017\n",
      "file                 0.016\n",
      "the                  0.015\n",
      "letter               0.014\n",
      "\n",
      " topic 4's tokens:\n",
      "term                 frequency\n",
      "\n",
      "to                   0.111\n",
      "motion               0.049\n",
      "of                   0.036\n",
      "the                  0.028\n",
      "notice               0.016\n",
      "defendant            0.015\n",
      "by                   0.015\n",
      "for                  0.014\n",
      "plaintiff            0.014\n",
      "order                0.013\n"
     ]
    }
   ],
   "source": [
    "#try looking at different topics' constitutinos\n",
    "for i in range(5):\n",
    "    print(\"\\n topic {}'s tokens:\".format(i))\n",
    "    explore_topic(topic_number = i)\n",
    "#seems like topic:\n",
    "#0: \n",
    "#1: \n",
    "#2: \n",
    "#3: \n",
    "#4: \n",
    "#etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from gensim.models import Phrases #seems like this is slower, but Phaser was not compatible to our code? need some research\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_description(review_text, min_topic_freq = 0.05):\n",
    "    \"\"\"\n",
    "    accept the original text of a review and \n",
    "    1. parse it with spaCy,\n",
    "    2. apply text pre-proccessing steps, \n",
    "    3. create a bag-of-words representation, \n",
    "    4. create an LDA representation, and\n",
    "    5. print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the review text with spaCy\n",
    "    parsed_review = nlp(review_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_review = [token.lemma_ for token in parsed_review if not punct_space(token)]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "    # remove any remaining stopwords\n",
    "    trigram_review = [term for term in trigram_review if term not in STOP_WORDS]\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    review_lda = lda[review_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    review_lda.sort(key = lambda tup: tup[1], reverse = True)\n",
    "    \n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        print('{:25} {}'.format(topic_names[topic_number], round(freq, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
